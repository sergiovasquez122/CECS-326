--------------------------------Process------------------
A process is a running program. A program is just
a bunch of instructions and some data sitting in disk.It
is an operating system which brings it to life.

Often we are running more than one program at once.
examples code be listening to music while having
your browser open and using your favorite IDE.

In order to virtualize the CPU to give the illusion
of many CPU's the OS applies time-sharing of the CPU.
We run one program then stop it and run another one and
so on.

mechanisms: low-level methods that implement a 
needed piece of functionality.

address pspaces is the memory that a process can 
address. part of the machine state of a process are
registers.

parts of the machine state:
program counter otherwise known as an instruction pointer
this is what tells us which instrucdion of the program will execute next

stack pointer

A thread is the unit of execution within a process. A process can have
anywhere from just one thread to many threads.

	process is composed of threads

-----------------------------Process State---------------
-As a process executes, it changes state.
-The state of a process is defined in part by the current activity of that process

	Each process may be in of the following states
	
	New - The process is being created

	Running - Instructions are being executed

	Waiting - The process is waiting for some event to occur(I/O completion or reception of a signal)

	Ready	- The process is waiting for waiting to be assigned to a processor. 

	Terminated - The process has finished execution


	new ->  ready -> running -> terminated
		|		|
		|		|
		------	waiting


-----------------------------Process Control Block------
-Each process is represented in the operating system as a process control block


	process state
	program number
	program counter
	registers
	memory limits
	list of open files
	...

-process id: unique number to identify process

-process state: the particular state in which the process is at that particular moment

-program counter: indicates the address of the next instruction to be executed by the program

-cpu registers: registers being used by a particular process

-cpu scheduling: priority of the process

-memory management information: memory that is being used by a particular process

-accounting information: keep account of resources being and so on.

-I/O status information: I/O devices that are being assigned to the process

-----------------------------Process Scheduling----------
-The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization

-The objective of time sharing is to switch the CPU among processes so frequently that users can interact
with each program while it is running.

-To meet these objectives, the process scheduler selects an available process for program execution on the CPU.
	-For a single-processor system, there will never be more than one running process
	-If there are more processes, the rest will have to wait until the CPU is free and rescheduled


Scheduling queues

job queue: As processes enter the system, they are put into a job queue. Which consist of all processes in the system.

ready queue: The process that are residingin main memory and are ready and waiting to execute are kept on a list
called the ready queue.

----------------------------CPU SCHEDULING--------------
CPU scheduling is the basis of multi-programmed operating systems

By switching the CPU among processes, the operating system can make the computer more productive.

-In a single-processor system, only one process can run at a time.

-Any others must wait until the CPU is free and can be recheduled

-The objective of multi-programming is to have some process running at all times, to maximize CPU utilization

-A process is executed until it must wait, typically for the completion of some I/O request.

In a simple computer system, the CPU then just sits idle. All this waiting time is wasted; No useful work is accomplished.

-With multi-programming, we try to use the time productivly
	-several processes are kept in memory at a time

-When one process has to wait, the operating system takes the CPU away from that prcoess and gives the CPU to another process and this pattern continues.

----------------------------CPU and I/O burst cycles--------
-Process execution consists of a cycle of CPU execution and I/O wait.
	-Processes alternate between these two states

Process execution begins with a 
	CPU burst
		IO Burst
			CPU Burst
				IO Burst
					...

CPU burst is when the process is being executed in the 
CPU.

I/O burst is when the CPU is waiting for I/O for
further execution.

Eventually, the final CPU burst ends with a system request to terminate execution.

load store
add store
read from file

wait for I/O

store increment
index
write to file

wait for I/O

load store
add store
read from file

wait for I/O

----------------------------Preemptive and non-preemptive scheduling-------------------------------------

		CPU scheduler
Whenever the CPU becomes idle, the operating system must
select on of teh process in the ready queue to be executed. The selection process is carried out by the short-term scheduler. The scheduler selects a process from teh processes in the memory that are ready to execute and allocats the CPU to that process.

		Dispatcher

The dispatcher is the module that gives control
of the CPU to the process selected by the short-term
scheduler. The time it takes for the dispatcher
to stop one process and start another running is known
as the dispatch latency.

CPU-scheduling decisions may take place under the following four circumstances:

1. When a process switches from the running state to the waiting state

2. When a process switches from the running state to the ready state(ex, when an interrupt occurs.)  

3. When a process switches from the waiting state to the ready state(ex, at completion of I/O) 

4. When a process terminates.

No choices for situations for 1 and 4. A new process must be selected for execution.
However, there is a choice for situations 2 and 3.

When scheduling takes place only under circumstances 1 and 4, we say that scheduling scheme is nonpremptive or cooperative; otherwise, it is preemptive.

----------------------------Scheduling Criteria---------------

cpu utilization
	throughput
		turnaround time
			waiting time
				response time

cpu utilization: we want to keep the cpu as busy as
possible. conceptually cpu utilization can range from
0 to 100 percent. In a real system, it should range from 40 percent to 90 percent.

throughput: If the cpu is busy executing processes, then work is being done. One measure of work is the number of processes that are completed per time unit, called throughput.

turnaround time: From the point of view of a particular
process, the important criterion is how long it takes to
execute that proces. the interval from time of submission
of a processs to the time of completion is the turnaround time.
turnaround time is the sum of the periods spent waiting
to get into memory, waiting in the ready queue, executing
on the CPU and doing I/O.

waiting time: The cpu scheduling algorithm does
not affect the amount of time during which a process
executes or does I/O. It affects only the amount of
time that a process spends waiting in the
ready queue. Waiting time is the sum of the
periods spent waiting in the ready queue.

response time: turnaround time may not be the best critertion. another measure is the time from the submission of a request until the first response is produced. this measure, called response time, is the time it takes to start responding, not the time it takes to output the responses. The turnaround time is generally limited by the speed of the output device.

----------------------------Scheduling Algorithms-----------
(first-come, first-served scheduling)

-by far the simplest CPU-scheduling algorithm.

-the process that request the CPU first is allocated the CPU first.

-The implementation of the FCFS policy is easily managed
with a FIFO queue

-first-in first out.

-when a process enters the ready queue, its pc is linked onto the tail of the queue.

-when the cpu is free, it is allocated to the process at the head of the queue.

-the running process is then removed from the queue.

-the average waiting time under the FCFS policy, however, is often quite long.

consider the following set of processes that arrive at time 0

	process		burst time(ms)
	p1		24
	p2		3
	p3		3

If the process arrives in the order p1, p2, p3 and are served in fcfs order, we get the result in the following gantt chart

(0 + 24 + 27) / 3 = 17.0


If the process arrives in the order p2, p3, p1 and are served in fcfs order, we get the result in the following gannt chart

(0 + 3 + 6) / 3 

this reduction is substantial. Thus, the average waiting time under an FCFS policy is generally not minimal and may vary substantially if the process's CPU burst time vary  greatly.

		the FCFS scheduling algorithm is nonpremptive

-Once the CPU has been allocated to a process, that process keps the CPU until is has either releases teh CPU, eiteher by terminating or by requesting I/O.

-The FCFS algorithm is thus particulary troublesome for time-sharing systems, where it is important that each user get a share of the CPU at regular intervals.

-It would be disatrous to allow one process to keep the CPU for an extended period.

----------------------------Solved problem on First-Come, First-Served scheduling------------------------------------
			convoy effect
If processe with higher burst time arrived before the processes with smaller burst time, then, smaller processes have to wait for a long time for longer processes to release the CPU.

process		arrival time		burst time
p1			4			5
p2			6			4
p3			0			3
p4			6			2
p5			5			4

when two processes arrive at the same time it is the process that arrive first that will start first,
thus the job sequence is. 
<p3, p1, p5, p2, p4>

turnaround time = completion time - arrival time
(TA)

TA(p1) = 9 - 4 = 4
TA(p2) = 17 - 6 = 11
TA(p3) = 3 - 0 = 3
TA(p4) = 19 - 6 = 13
TA(p5) = 13 - 5 = 8

(5 + 11 + 3 + 13 + 8) / 5 = 8.0

waiting time = turn-around time - burst time
(WA)

WA(p1) = 5 - 5 = 0
WA(p2) = 11 - 4 = 7
WA(p3) = 3 - 3 = 0
WA(p4) = 13 - 2 = 11
WA(p5) = 8 - 4 = 4

(0 + 7 + 0 + 11 + 4) / 5 = 4.4

----------------------------Shortest Job-first scheduling----------------------	
-This algorithm associates with each process the length of the process's next CPU burst.

-When the CPU is available, it is assigned to the process
that has the smallest next CPU burst.

-If the next CPU burst of two processes are the same, FCFS scheduling is used to break the tie.

	The SJF algorithm can be either preemptive or nonpreemptive

	A more appropiate term for this scheduling method would be the shortest-next-CPU-Burst algorithm
because scheduling depends on the length of the next CPU burst of a process, rather than its total length.

example of SJF scheduling (non-preemptive)

process Id		burst time
p1			6
p2			8
p3			7
p4			3

(0 + 3 + 9 + 16) / 4 = 7.0

by comparison, if we were using the FCFS scheduling scheme, the average waiting time would be 10.25

(0 + 6 + 14 + 21) / 4 = 10.25

example of SJF scheduling (preemptive)

Consider the follwing four processes, with the length of the CPU brust given in milliseconds and the processes arrive at the ready queue at the times shown

process		arrival time		burst time
p1		0			8
p2		1			4
p3		2			9
p4		3			5

p1 one executes for one unit of time
p2 comes at time 1 and takes hold of CPU
p3 comes at time 2 and is in ready queue
p4 comes at time 5 and is in ready queue
p2 terminates and now p4 is scheduled to be executed.
p4 ends terminates at time 10 and gives p1 time to complete.
p1 executes until time 17 and then terminates and give
execution to p3.
p3 terminates at time 26

waiting time = total waiting time - no of milliseconds process executed - arrival time.

waiting time for P1 = (10 - 1 - 0) = 9
waiting time for p2 = (1 - 0 - 1) = 0
waiting time for p4 = (5 - 0 - 3) = 2
waiting time for p3 = (17 - 0 - 2) = 15

(9 + 0 + 2 + 15 ) / 4 = 6.5

preemptive SJF scheduling is sometimes called shortest-time remaining job scheduling

problems with SJF scheduling

the difficult with the SJF algorithm is knowing the length of the next CPU request.

although the SJF algorithm is optimal, it cannot be implemented at the level of short-term CPU scheduling

there is no way to knwo the length fo the next CPU burst

One approach is 
	-to try to approximate the SJF scheduling
	-we may not know the length of the next CPU burst but we may be able to predict its value.
	-we expect that the next CPU burst will be similar in length to the previous ones.

	-Thus, by computing an approximation of the lenght of the next CPU burst, we can pick the process with the shortest predicted CPU burst.

----------------------------SJF Scheduling---------------
Process		Arrival time		Burst time
p1		0			12
p2		2			4
p3		3			6
p4		8			5

waiting time = completion time - processes execution time - arrival time

WA(p1) = (17 - 2 - 0) = 15
WA(p2) = (2 - 0 - 2)  = 0
WA(p3) = (6 - 0 - 3) = 6
WA(p4) = (12 - 0 - 4) = 8

(15 + 0 + 3 + 4) / 4 = 5.5

The average waiting time is 5.5 

-----------------------SFJ scheduling solved problem----------------------
Process		Arrival Time		Burst Time
p1		0			8
p2		3			6
p3		7			1
p4		8			3

turn around time = completion time - arrival time 
(TA)

TA(p1) = 20 - 0 = 20
TA(p2) = 10 - 3 = 7
TA(p3) = 8 - 7 = 1
TA(p4) = 13 - 8 = 5

(20 + 7 + 1 + 5) / 4 = 8.25

----------------------------Priority Scheduling-----------
-A priority is associated with each process, and the CPU is allocated to the process with the highest priority 

-Equal-priority processes are scheduled in FCFS order.

-An SJF algorithm is simply a priority algorithm where the priority is the inverse of the (predicted) next CPU burst. The larger the CPU burst, the lower the priority, and vice versa.

SJF is basically a priority scheduling algorithm

	Priority Scheduling can be either preemptive or nonpreemptive.

A preemptive priority scheduling algorithm will preempt the CPU if the priority of the newly arrived process is higher than the priority of the currently running process.

A nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue.

Consider the following set of processes, assumed to have arrived at time 0, in the order p1, p2, p3, p4, p5, with the length of the CPU burst given in milliseconds.

process		burst time	priority
p1		10		3
p2		1		1
p3		2		4
p4		1		5
p5		5		2

smaller the number the higher the priority

-A major problem with priority scheduling algorithms is indefinite blocking, or starvation.
-A process that is ready to run but waiting for the CPU can be considered blocked.
-A priority scheduling algorithm can leave some low priorty processes waiting indefinitely.
-In a heavily loaded computer system, a steady stream of higher-priority processes can prevent a low-priority process from ever geting the CPU.

-a solution to the problem of indefinite blockage of low-priority processes is aging.
-aging is a technique of gradually increasing the priority of processes that wait in the system for a long time.

for example
-if priorities range from 127(low) to 0(high), we could increase the priority of a waiting process by 1 every 15 minutes.
-eventually, even a process with an initial priority of 127 would have the highest priority in the system and would be executed.

----------------------------Priority Scheduling solved problem----------------------------

Process		Arrival		Burst		Priority
p1		0		11		2
p2		5		28		0
p3		12		2		3
p4		2		10		1
p5		9		16		4

WA(p1) = 40 - 2 - 0 = 38
WA(p2) = 5 - 0 - 5 = 0
WA(p3) = 49 - 0 - 12 = 37
WA(p4) = 33 - 3 - 2 = 28
WA(p5) = 51 - 9 = 42

---------------------------Priority Scheduling solved problem-------------------------------
process		arrival		burst 		priority
p1		0		4		2
p2		1		3		3
p3		2		1		4
p4		3		5		5
p5		4		2		5

turn around time = completion - arrival
waiting time = turn around - burst

ta(p1) = 4 - 0 = 4
ta(p2) = 15 - 1 = 14
ta(p3) = 12 - 2 = 10
ta(p4) = 9 - 3 = 6
ta(p5) = 11 - 4 = 7

(4 + 14 + 10 + 6 + 7) / 5 = 8.2

wa(p1) = 4 - 4 = 0
wa(p2) = 14 - 3 = 11
wa(p3) = 10 - 1 = 9
wa(p4) = 6 - 5 = 1
wa(p5) = 7 - 2 = 5

(0 + 11 + 9 + 1 + 5) / 5 = 5.2

---------------------------Scheduling Algorithms---------
-The round-robin scheduling algorithm is designed especially for timesharing systems.
-It is similar to FCFS scheduling, but preemption is added to switch between processes.
-A small unit of time, called a time quantum or time slice, is defined

-the ready queue is treated as a circular queue

-the CPU scheduler goes around the ready queue, allocating the CPU to each process for a time interval of up to 1 time quantum.

-we keep the ready queue as a FIFO queue of processes

-New processes are added to the tail of the ready queue

-The CPU scheduler picks the first process from the ready queue, sets a timer to interrupt after 1 time quantum, and dispatches the process.

		One of two things will then happen

the process may have  a cpu				the cpu burst of running process is longer than 1 time quantum, the timer will go off and will cause an interrup to the o.s
a context switch will be executed, and the process will be put at the tail of the ready queue.
the cpu scheduler will then select the next process in the ready queue
burst of less than 1 time quantum

the process will release the cpu voluntarily

the cpu scheduler will then proceed to the next
process in the ready queue


----------------------------Round-Robin scheduling--------

calculating turnaround time and waiting time for Round-robin scheduling

Consider the following set of professes taht arrive at time 0, with the length of teh CPU burst given in milliseconds and tiem quantum taken as 4 milliseconds for RR scheduling

process ID 	burst 
P1		24
P2		3
P3		3

turn-around time = completion time - arrival time
wait time = turn-around time - burst time

30 - 0 = 30
7 - 0 = 7
10 - 0 = 10

(30 + 7 + 10) / 3

30 - 24 = 6
7 - 3 = 4
10 - 3 = 7

(6 + 4 + 7) / 3

Consider the set of 5 processes whose arrival time and burst time are given below:

process	arrival burst
p1	0	5		
p2	1	3
p3	2	1
p4	3	2
p5	4	3

If the CPU scheduling policy is Round-Robin
with time quantum = 2 units, calculate the 
average waiting time and average turn around time.

(13 + 11 + 3 + 6 + 10) / 5 = 8.6

(8 + 8 + 2 + 4 + 7) / 5

----------------------------Context Switch---------------
-Interrupts cause the operating system to change a CPU from its current task to run a kernel routine.

-Such operations ahhappeon frequently on general-purpose systems.

-when an interrupt occurs, the system needs to save teh current contex of the process currently running on the CPU
so that it can restore that context when its processing is done, essentially suspending the process and then
resuming it

-the contex is represented in the PCB of the process

example: reading book and then mother tells you do something. mother is interupt, need to interrupt your current
task and do the new task. put bookmark on current page and then do the other task. by putting bookmark you
are saving current context and when you are done you will resume reading of book.

-switching the CPU to another process requires performing a state save of the current process and a state restore
of a different process.

	this task is known as a context switch.

-Context-switch time is pure overhead, because the system does no useful work while switching.

-It speed varies from machine to machine, depending on the memory speed, the number of registers that must
be copied and the existence of special instructions.

-typical speeds are a few milliseconds.

-----------------------Process Creation------------------------------
-A process may create several new processes, via a create-processes system call, during the course of execution.

-The creating process is called a parent process, and the new processes are called the children of that process

-Each of these new processes may in turn create other processes, forming a tree of processes.

-when a process creates a new process, two possibilities exist in forms of execution

1. the parent continues to execute concurrently with its children

2. the parent wait until some or all of its children have terminated.

there are also two possibilities in terms of the address space of the new process

	1. the children process is a duplicate of the parent process.

	2. the children process has a new program loaded into it.


---------------------------Operations on processes-----------------------
-A process terminates when it finished executing its final statement and asks the operating system to delete it using
the exit() system call.
-At that point, the process may return a status value(typically an integer) to its parent process(vai the wait() system call).
-All the resources of the process-including physical and virtual memory. open files and I/O buffers/are deallocated by
the operating system.
-Terminations can occur in other circumstances as well:
---A process an cause the termination of another process via an appropiate system call
---Usually, such a system canll can be invoked only bythe parent of the process that is to be terminated
---otherwise, users could arbitrarily kill each other's job.

---A parent may terminate the execution of one of its children for a variety of reasons, such as these

-----The child has been exceeded it usuage of some of the resources that it has been allocated.

-----The task assigned to the child is no longer required

-----The parent is exiting, and the operating system does not allow a child to continue if its parent terminates

----------------------------Interprocess communication------------------
Processes executing concurrently in the operating system may be either independent processes or coooperating processes

independent processes: they cannot affect or be affected by the other processes executing in the system

cooperating processes: they can affect or be affected by the other processes executing in the system.

any process that shares data with other processes is a cooperating process.

there are several reasons for providing an enviroment that allows process cooperation

	information sharing

	computation speedup

	modularity

	convenience

cooperating process requires an interprocess communication mechanism that will allow them to exchange data
and information

there are two fundamentals of interprocess communication

	1. shared memory
	2. message passing

-in the shared memory model, region of memory that is shared by cooperating process is established
processes can then exchange information by reading and writing data to the shared region.

-In the message passing model, communication takes place by means of message exchanged between the cooperating processes.


-----------------------------Shared Memory Systems---------------------
-interprocess communication using shared memory requires communicating processes to establish a region of shared
memory.
-typically, shared-memory region resided in the address space of teh process creating the shared-memory segment.
-other process that wish to communicate using this shared memory segment must attach it to their
address space
-normally, the operating system tries to prevent one process from accessing another process's memory
-shared memory requires that two or more processes agree to remove this restriction.

producer-consumer problem

A producer processs produces information that is consumed by a consumer process

-one solution to the producer-consumer problem uses shared memory.

-to allow producer and consumer processes to run concurrently, we must have available a buffer of items that
can be filled by the producer and emptied by the consumer.

-this buffer will resided in a region of memory that is shared by the producer and consumer processes

-the producer and consumer must be syncronized, so that the consumer does not try to consumer an item that
has not yet been produced.

-----------------------------Message-passing systems-----------------------------

Message-passing a mechanism
to allow processes to communicate and to synchronize
their actions without sharing the same address space
and is particulary useful in a distributed environment, where
the communicating processs may reide on different computer connected by a network.

provides at least two operations

-send messages to other process

-receive message from other process

message sent by a process can be of either fixed or variable size.

fixed size:
system-level implementation is straightforward
task of programming is more difficult

variable size:
requires a more complex system-level implementation.
but the programming tasks become simpler.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

If processes P and Q want to communicate, they must send message to and 
receive message from each other.

A communication link must exist between them.

link can be implemented in many ways

-direct or indirect communications
-synchronous or asynchronous communication
-automatic or explicit buffering

issues
naming
synchronization
buffering

-----------------message passing systems-----------------
processes that want to communicate must have a way to refer to each other.
they can use either direct or indirect communication

under direct commnication-each process that want sto communicate must
explicitly name the recipient or sender of teh cummnication.

send(P, message) - send a message to process P
receive(P, message) - receive a message from process Q

A communication link in this scheme has the following properties:
A link is established automatically between every pair of processes that
want to communicate. The processes need to know only each other's identify 

A link is associated with exactly two processes.

Between each pair of processes, there exists exactly one link.

symmtry in addressing.

-----------------------------Process API-----------------

must be included in the interface of any OS.

Create: OS must have methods to create new processes.
when you double click on a application icon the os
is invoked to create a new process to run the program.
Destroy: OS must have methods to terminate processes.
Wait: OS must hav emthods to wait for process to stop running.
Miscellaneous Control: Methods such as suspending
a process and resuming it.
Status: Methods to get the status information of 
a process.

--------------------------Process States------------------
A process can in different states during its lifetime.
A process can be in one of three states:

- Running: In the running state, a process is running on  a processor. This means it is executing instructions.

- Ready: In the ready state, a process is ready to run but the OS has not choosen to let it run at the given moment.

- Blocked: Process has performed some operation that doesn't allow it continue until some event has taken place.


Process state of two processes that only use the CPU

time		Process0		Process1
1		Running			Ready
2		Running			Ready
3		Running			Ready
4		Running			Ready
5		Done			Running
6		Done			Running
7		Done			Running
8		Done			Running


------------------------Process API----------------------

the fork system call is to create a new process.
It create an almost exact copy ofthe calling process.
the process comes into life as if it had been called
fork() itself.

----------------------Threads----------------------------
threads ia like a separate process execept they share
the same address space and thus can access the same data.

One reason we would want to use threads is parallelism.
you can speed up a program by using multiple threads 
to do a portion of the work.

Another reason is to avoid blocking program progress
due to slow I/O.  While one thread in your program waits
another thread can be switched to which are ready to run.
threads share an address space and thus it is easy
to share data.

working with threads make application more rich but
also more complicated.

race condition: the result depends on the timing
execution of the code.

critical section: piece of code that accesses a share
variable and must not concurrently executed by
more than one thread.

a critical section is piece of code that accesses
a shared resource, usually a variable or data structure.

a race condition: arises if multiple
thread of executation enter the critical section at
roughly the same tiem. both
attempt to update the shared data structure, leading
to unforseen results.

mutual exclusion:
the proprerty that if one thread is executing
within the critical section no other thread is allowed
to do so.

the operating system was the first concurrent program.
-------------------------synchronization-------------------
A cooperating process is one that can affect or be affected by other processes executing in the system.

                        cooperating processes can either

                       /                                \
                      /                                  \
                     /                                    \
                    /                                      \
                   /                                        \
                directly share a logical address             or be allowed to share data only through files or messages
                space. (both code and data)

the problem is that concurrent access to shared data may result in data inconsistency. If multiple processes try to 
access and modify the same data it could lead to data inconsistency. This is why we need process sychronization.

Producer-Consumer problem
A producer process produces information taht is consumed by a consumer process.

For example, a compiler may produce assembly code, which is consumed by an assembler.
The assembler, in, may produce object modules, which are consumed by the loader.

-One solution to the producer-consumer problem uses shared memory.

-To allow producer and consumer processes to run concurrently, we must have available a buffer of items that
can be filled by the producer and emptied by the consumer

-This buffer will reside in a region of memory that is shared by the producer and consumer processes

-A producer can produce one item while the consumer is consuming another item

-The producer and consumer must be sychronized, so that the consumer does not try to consume one item that has not yet
been produced


                                    two kinds of buffers


        Unbounded buffer                                        Bounded Buffer
                |                                                       |
                |                                                       |    
                |                                                       |
                |                                                       |
                |                                                       |
        Places no practical limit                                Assumes a fixed buffer size. In this
on the size of the buffer. the consumer may                      case, the consumer wait wait if the
have to wait for new items, but the producer                     buffer is empty, and the producer must wait if the buffer is b full.

counter variable = 0
counter will keep track of the number of items in the buffer

counter is incremented every time we add a new item to the buffer counter++
counter is decremented every time we remove one item from the buffer counter--

example:
-Suppose that the value of the vaiable counter is currently 5
-The producer and consumer processes execute the statement "counter++" and "counter--" concurrently.

-following the execution of these two statements, the value of the variable counter may be 4, 5, or 6

-the only correct result, though is counter == 5, which is generated correctly if the poducer and consumer execute separately

counter++ may be implemented in machine language as:

        register_1 = counter // value of register_1 is equal to 5
        register_1 = register_1 + 1 // value of register_1 is equal to 6
        counter = register_1 // value of counter is 5

counter-- may be implemented in machine language as:

        register_2 = counter // value of register_2 is equal to 5
        register_2 = register_2 - 1 // value of register_2 is equal to 4
        counter = register_2 // value fo counter is 4

table of how the process may take place

t0      producer        execute     register_1    =   counter                  {register_1=5}
t1      producer        execute     register_1    =   register_1 + 1           {register_1=6}
t2      consumer        execute     register_2    =   counter                  {register_2=5}
t3      consumer        execute     register_2    =   register_2 - 1           {register_2=4}
t4      producer        execute     counter       =   register_1               {counter=6}
t5      consumer        execute     counter       =   register_2               {counter=4}

we arrived at this incorrect state because we allowed both processes to run concurrently.

A situation where several processes access and manipulate the same data concurrently and the outcome of the
execution depends on the particular order in which the access takes place, is called a race condition.

We want the resulting changes not to interfere with one another, hence we need process synchronization.

don't want two or more processes to access and manipulate the same data and then causing some kind of 
inconsistency.

---------------------------------------------The Critical Section-Problem--------------------------------------
Process sychronization is another topic in operating system. critical section-problem is related to process
sychronization.

consider a system consisting of n processes (p0, p1, ..., pn)
each process has a segment of code, called a 
			critical section.
in which the process may be changing common variables, updating a table, writing a file, and so on.


when one process is executing in its critical section, no other process is to be allowed to execute in its critical
section. that is, no two other processes are executing in their critical sections at the same time.
that is, no two processes are executing in their critical sections at the same time.

the critical-section problem is to design a protocol that the processes can use to cooperate. they should not
try to manipulate shared data concurrently.

the critical-section problem is to design a protocol that the processes can use to cooperate.

-Each process must request permission to enter its critical section
-The section of code implementing the request is the entry section
-The critical section may be followed by an exit section
-The remaining code is the remainder section

general structure of a process

do{
    entry section
        critical section
    exit section
        remainder section
} while(true);

A solution to the critical-section problem must satisfy the following three requirements:

1. Mutual Exclusion:

If process Pi is executing in its critical section, then no other processes can be executing in their critical section

2. Progress

If no process is executing in its critical section and some processes wish to enter their critical section, then only
those processes that are not executing in their remainder sections can be participate in the decision on which will
enter its critical section nexts, and this selection cannot be postponed indefinitely.

3. Bounded waiting

There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after
a process has made a request to enter its critical section and before that request is granted.
not having this would lead to starvation.

----------------------------------------Peterson Solution---------------------------------
-A classic software-based solution to the critical-section problem
-May not work correctly on modern computer architecture
-Help us to understand the problem at a software implementation level.

Peterson's solution is restricted to two processes that alternate execution between their critical sections
and remainder sections. Let's call the processes Pi and Pj.

Peterson's solution requires two data items to be shared between two processes.

int turn						boolean flag[2]
   |								|
   |								|
   |								|
Indicates whose turn it is to				Used to indicate if a process is ready to enter its
enter its critical section 				critical section.

examples:
turn == i tells us it is process_i turn 		flag[j] == true tells us process j is ready 
to enter its critical section.				to enter its critical section.


Peterson's solution is like a humble algorithm, when it has the chance to enter the critical section it 
gives the turn to the other process. 

analogy:
you and your friend are waiting for the bus and then when it comes you allow you friend to go first and wait
until he enters the bus before you yourself enters the bus.

structure of pi peterson solution

do{
	flag[i] = true; // Process pi is ready to enter its critical section
	turn = j; // algorithm is a humble algorithm so will give turn to process pj
	while(flag[j] && turn[j] == j); // wait while process pj is in its critical section
	// has entered its critical section
	critical section

	flag[i] = false; // set the flag as false as the process pi has completed its critical section.

	remainder section
} while(true);


structure of process pj in peterson's solution

do{
	flag[j] = true;// process pj wants to enter its critical section
	turn = i; // process pj is humble and offer the turn to process pi
	while(flag[i] && turn == i); // process pj will wait for process pi 

	critical section

	flag[j] = false; // done with the critical section

	remainder section
} while(true);


example: 
both try to enter the critical section at the same time.
process pi enters first and sets its flag[i] to true and give the turn to process pj.

process pj enters seconds and sets if flag[j] to true and give the turn to process pi.

since turn == i and flag[i] == true, process pi will enter its critical section.
flag[i] will becomes false once process pi exits its critical section so now 
process pj is allowed to enter its critical section.


The peterson solution satisfies all three requirements for the critical section problem

1. mutual exclusion is preserved

2. process is preserved

3. bounded waiting is preserved

peterson solutions is only restricted to two processes and is not guaranteed to work on modern computer
architecture


--------------------------------------Test and Set Lock--------------------------------------------
-Hardware solution to the synchronization problem
-There is shared lock variable which can take on values either, 0 or 1.
-0 means unlocked and 1 means locked
-Before entering into the critical section, a process inquires about the lock.
-If it is locked, it keeps on waiting till it becomes free
-If it is not locked, it sets the lock and executes the critical section

example:
room that only one person can be in at a time. checks if the room is locked or unlock.
only when person who is currently in the room and finishes work unlocks the room can another person
use the room.

definition of test_and_set() function.

boolean test_and_set(boolean* target){
	boolean rv = target;
	*target = TRUE;
	return rv;
}

this function is an atomic operation. all the 
instruction are done and cannot be interrupted.

example:

initially the lock is set to zero.

process pi executes first.
it call the test_and_set function on the lock.
since the lock is 0 the return while will be zero and
it will continue onto the critical section. process pj
will wait in the while loop until process pi exits the critical section
and sets the lock value back to 0.

process pi

do {
	while(test_and_set(&lock));

	// do nothing
	// critical section
	lock = false;
	//remainder section	
} while(true);

advantage of current solution

satisfies mutual-exclusion.

does not satisfies bounded waiting.

process pi enters and complete critical section while process pj waits.
when process pi is done another process pk enters and now process pj still have to waits.
this could happen indefinitely.leads to starvation of process to critical section.

---------------------------------------Semaphore---------------------------------------------------
-Software based solution to the sychronization problem.

-Semaphore proposed by Dijkstra, is a technique to manage concurrent processes by using a simple integer value,
which is known as a semaphore.

-Semaphore is simply a variable which is non-negative and shared between threads. this variable is used to solve
the critical secion problem and to achieve process synchronization in the multi-processing environment

-A semaphore S is an integer variable that, apart from initialization is accessed only through two
standard atomic operations: wait() and signal();

wait() -> [from the Dutch word proberen, which means 'to test']
signal() -> [from the Dutch word verhogen, which means 'to increment']

definition of wait();

P(Semaphore s){
	while(s <= 0); // stay in while loop while semaphore resources are used up
	// no operation
	s-- // decrement a resource from the semaphore.
}

if the value of s > 0 greater than 0, then s will be decremented and enter the critical section.
it decrements the value of s to let other other processes to know that the value of the semaphore has been decreased.

definition of signal();
V(Semaphore s){
	s++; // process is done using the resource
}

to tell other processes that the process is done using the resource.


all the modification to the integer value of the semaphore in the wait() and signal() operations
must be executed indivisbly, 
that is, when one process modifies the semaphore value, no other process 
can simulatenously modify that same semaphore value.

1. Binary Semaphore
The value of a binary semaphore can range only between 0 and 1. On some systems, binary semapohres are known as mutex locks,
as they are locks that provide mutual exclusion.

the value of a semaphore is initialized to 1.

2. Counting Semaphore
Its value can range over an unrestricted doman. It is used to control access to a resource that
has multiple instances. 

two instances of 2 for resource with count of 2
S = 2

process 1 will enter the loop and then decrement the value of S to 1

process 2 will enter the loop and then decrement the value of S to 0

process 3 will enter the loop and then will stay in loop until process 1 or
process 2 release the resource.

-------------------------------------Disadvantage of Semaphores-------------------------------------------------
-The main disadvantage of the semaphore definition that was discussed is that it requires busy waiting.

- While a process is in its critical section, any other process that tries to enter its critical section
must loop continously in the entry code.

-busy waiting wastes CPU cycles that some other process might be able to use productively.

-this type of semaphore is called a spinlock because the process 'spins' while waiting for the lock.

to overcome the need for busy waiting, we can modify the definiton of the wait() and signal() semaphore operations.

-When a process executes the wait() operations and finds that the semaphore value is
not positive, it must wait.

--however, rather than engaging in busy waiting, the process can block itself.

--The block operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to the waiting state.

--the control is transferred to the CPU schedule, which select another process to execute.

--------------------------------------Deadlocks and Starvation--------------------------------------
-The implementation of a semaphore with a waiting queue may result in a situation where
two or more processes are wiaitng indefintey for an event that can be caused only one of the 
waiting processes.
-The event in question is the execution of a signal() operation. When such as state is reached, these processes are said
to be deadlocked.

p0			p1	
wait(S);		wait(Q);
wait(Q);		wait(S);
   .			   .
   .			   .
   .			   .
signal(S);		signal(Q);
signal(Q);		signal(S);

---------------------------------------------Classic Problems of Syncrhonization-------------------------------------
					          (The bounded-buffer problem)

The bounded buffer problem(producer consumer problem) is one of the classics problems of synchronization
Problems are used to determine how solutions are able to solve problems and how well are they
able to do it.

global variables
int n; // pool consists of n buffers
semaphore mutex = 1;// mutual access to critical section
semaphore empty = n;// count number of empty buffers
semaphore full = 0;// count number of full buffers

There is a buffer of n slots and each slot is capable of storing one unit of data.

There are two processes running, namely, producer and consumer, which are operating
on the buffer.

-there are two processes running, namely, producer and consumer, which are operating on the buffer

operations of the processes
-the producer tries to insert data into an empy slot of the buffer.
-the consumer tries to remove data from a filled slot in the buffer.
problems we must take care of
-the producer must not insert data when the buffer is full
-the consumer must not remove data when the buffer is empty.
-the producer and consumer should not insert and remove data simultaneously

we make use of three semaphores:

1. m(mutex), a binary semaphore which is used to acquire and release the lock
2. empty, a counting semaphore whose initial value is the number of slots in the buffer, since, initially all slots are empty.
3. full, a counting semaphore whose initial value is 0.  

// producer code

do{
	wait(empty); // wait until empty > 0 and then decrement empty
	wait(mutex); // acquire lock
	/* add data to buffer */
	signal(mutex); // release lock 
	signal(full); // increment full because the producer has added one data to the buffer
} while(TRUE);

// consumer code
do {
	wait(full); // wait until full > 0 and then decrement 'full'
	wait(mutex);// acquire lock
	/* add data to buffer */
	signal(mutex); // release lock
	signal(empty); // increment empty because the consumer has removed one data to the buffer
} while(TRUE);

three problems we have stated have been solved.
the producer should not produce when the buffer is full.
the consumer should not consume when the buffer is empty.
the consumer and producer should not simulatenously consume and produce.


-----------------------------------READER-WRITERS PROBLEM-------------------------------------------
-A database is to be shared among several concurrent processes.
-Some of these processes may want only to read the database, wherease others may want to
update
-We distinguish between these two types of processes by referring to the former as readers and the latters
as writers
-obviously, if two readers access the shared data simultaneously, no adverse affect will result.
-however, if a writer and some other thread(either a reader or a writer) access the database simulatenously,
chaos may ensue.
-To ensure that these difficulties do not arise, we require that the writers have
exclusive access to the shared database.
this sychronization is referred to as the readers-writers problem.

we will make use of two semaphores and an integer variable.
1. mutex, a semaphore(initialized to 1) which is used to ensure mutual exclusion when readcount is updated i.e
when any reader enters or exit from the critical section.
2. wrt, a semaphore(initialized to 1) common to both reader and writer processes.2. wrt, a semaphore(initialized to 1) common to both reader and writer processes.
3. integer that keep tracks of how many variables are reading the data.

writer process
do{
	/* writer requests for critical section */
	wait(wrt);
	/* perform the write */
	// leaves the critical section 
	signal(wrt);
} while(true);

reader process
do{
	wait(mutex);
	readcnt++; // the number of readers has now increased by 1
	if(rcnd == 1)
		wait(wrt); // this ensure no writer can enter if there is even one reader
	signal(mutex); // other readers can enter while this current reader is inside teh crticial section
/*	current reader perform reading here */
	wait(mutex);
	readcnt--;
	if(readcnt == 0)
		signal(wrt);
		signal(mutex);

} while(true);

---------------------------------The dining-philophers problem-------------------------------------
Another classic problem in Synchronization 

Five philosphers in two possible states, thinking state and eating state.
philospher needs to have two chopsticks in order to eat. chopsticks
are limited. 5 chopsticks for 5 philosphers.

				philospher either
			/			 	\
		      thinks 				eats
			|				 |
			|				 |
		   when a philospher			when a philospher gets hungry he tries to pick up the forks that are cloest to him, a philospher may pick up only one fork at a time. cannot pick up fork on the hands of a neighbor.
		   thinks he does not
		   interact with his colleagues.

no two adjacent philosphers can eat at the same time. this is a problem of resource allocation.
One simple soluition is to represent each fork/chopstick with a semaphore.
a philospher tries to grab a fork/chopstick by executing a wait() operation on that semaphore.
he releases his fork/chopsticks by executing the signal() operation on the appropiate semaphores.

when a hungry philospher has both his forks at the same time, he eats without releaseing his forks.
when he has finished eating, he puts down both of his forks and start thinking again.

One solution to represent each chopstick with a semaphore

A philosopher tries to grab a chopstick by executing a wait() operations on that semaphore.

He releases his fork/chopsticks by executing the signal() operations on the appropriate semaphores.

thus the shared data are
		semaphore[5];
where all the elements of chopsticks are initialized to 1.
so we are using binary semaphores.

structure of philospher i
do{
	// get the chopsticks adjacent to him
	wait(chopstick[i]);
	wait(chopstick[((i + 1) % 5)]); // uses modular because circular table
	// eat
	// release the chopsticks 
	signal(chopstick[i]);
	signal(chopstick[(i + 1) % 5]);
	// think
} while(TRUE);

although this solution guarantees that no two neighbors are eating simulatenously,
it could still create a deadlock.

suppose that all five philosphers become hungry simultaneously and each grabs their left chopstick.
All the elements of chopsticks will now be equal to 0.

When each philosopher tries to grab his right chopstick, he will be delayed forever.
this scenario will lead to a deadlock.

some possible remedies to avoid deadlocks

Allow at most four philosopher to sitting simultaneously on the table.

Allow a philosopher to pick up his chopsticks only if both chopsticks are available.

Use an asymmetric solution;that is, an odd philosopher picks up first his 
left chopstick and then hsi right chopstick, whereas an even philosopher picks
up his right chopsticks and then his left chopstick.


----------------------------------------Monitors----------------------------------------------------
-A high-level abstraction that provides a convenient and effective mechanism for process
sychronization
-A monitor type presents a set of programmer-defined operations that provide mutual exclusions
within the monitor.
-The monitor type also contains the declaration of variables whose values define the state of an instance of that type, along wit the bodies of procedures or functions that operare on that variables.


signal(mutex);
    ...
critical section
    ...
wait(mutex);
the code above will allow several processes to be 
executing in their critical sections simultaneously.

wait(mutex);
    ...
critical section
    ...
wait(mutex);

the process will be blocked on the second call.

syntax of a monitor

monitor monitor_name
{
//shared variable declaration
procedure p1(...){

}

procedure p2(...){

}

procedure pn(...){

}

initialization code(...){

}
}

-A procedure defined within a monitor can access only
those varaibles declared locally within the monitor and its parameter.
-Similary, the local variables of a monitor can accessed by only the local procedures.
-The monitor construct ensures that only one process at a time can be active within the monitor
-conditional construct-			condition x, y;
the only operations that can be invoked on a condition variable are wait() and signal().

the operation x.wait(); means that process invoking this operation is suspended until another process invokes
x.signal();

the x.signal() operations resumes exactly one suspended process.



queue		shared data				entry queue
associated
with x,y

		operations

		intialization code


Implementing a monitor using semaphores

wait(mutex);
    ...
body of F
    ...
if (next_count > 0)
    signal(next);
else 
    signal(mutex);

Implementing a condition variable using binary semaphore

x_count++;
if (next_count > 0)
    signal(next);
else
    signal(mutex);
wait(x_sem);
x_count--;

The operation x.signal() can be implemented as 
    if(x_count > 0) {
        next_count++;
        signal(x_sem);
        wait(next);
        next_count--;
}


resuming processes within a monitor

conditional-wait construct.

x.wait(c)

c = priority number.
when x.signal() is executed the process with the smallest
priority number is resumed next.


Dining-philosphes solution monitors 

-We now illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosphers problem.

-This solution imposesthe restriction that a philospher may pick up his chopsticks only if both are available.

-to code this solution, we need to distinguish among three states n which we may find a philospher.
for this purpose, we introduce the following data structures.

		enum {thinking, hungry, eating} state[5];

-philospher i can set the variable state[i] = eating only if his two neighbors are not eating:
(state[(i+4) % 5] != eating) && (state[(i+1) % 5] != eating)

-we also need to declare condition self[5]; where philospher i can delay himself when he is hungry 
but is unable to obtain the chopsticks he need.

Monitor dp{
	enum {thinking, hungry, eating} state[5];
	
	void pickup(int i){
		state[i] = hungry;
		test(i);
		if(state[i] != eating)
		self[i].wait();
	}

	void putdown(int i){
		state[i] = thinking;// done eating
		test((i + 4) % 5);// give the neighbors chance to eat.
		test((i + 1) % 5);
	}

	void test(int i){
		// if the philosopher to the left and right of a hungry philosopher then 
		// the philospher will start eating
		if((state[(i + 4) % 5] != eating) && (state[i] == hungry) && state[(i + 1) % 5] != eating)
		{
		state[i] = eating;
		self[i].signal(); // completed eating and now any other philosopher can start entering monitor
		}
	}

			
		// initially all philosphers are in the thinking state.
		initialization-code(){
			for(int i = 0;i < 5;i++){
				state[i] = thinking;
		}
	}
}

when one philospher finishes the philospher to the left and right are tested, this solution does not have
dead lock a philospher need to have both chopsticks avaiable before they are allowed to eat. only one process
at is allowed in the monitor at a time.

process synchronization question

Consider the methods used by processes P1 and P2 for accessing their critical secions whenever needed, as given below.
the initial values of shared boolean variables s1 and s2 are randomly assigned.

p1 methods
while(s1 == s2);
critical section
s1 = s2;

p2 method
while(s1 != s2);
critical section
s2 = not(s1)

Which one of the following statements describes the properties achieved?

1. mutual exclusion but not progress
2. progress but not mutual exclusion
3. neither mutual exclusion nor progress
4. both mutual exclusion and progress

this will achieve mutual exclusion but not progress.
one process can be postponed indefinitely.

Suppose s1 == s2 and process P1 wants to enter the critical section. If p2 does not want to enter critical 
section then P1 will wait indefinitely.

keep in mind definitions and analyze code to solve these problems.

question 2

The following program consists of 3 concurrent processes and 3 binary semaphores the semaphores are initialized to
s0 = 1, s1  = 0, s2 = 0

process p0 	process p1 		process p2
while(true){	wait(s1);		wait(s2);
wait(s0);	release(s0);		release(s0);
print(0);
release(s1);
release(s2);
}

How many times will proces p0 print '0'?

a. at least twice

b. exactly twice

c. exactly thrice

4. exactly once

a is the correct answer. did not take into account the case when

p0 runs then p1 then p0 then p2 then p0.

question

process p1
while(true){
wants1 = true;
while(wants2 == true);
wants1 = false;
} /* remainder section */

process p2
while(true){
wants2 = true;
while(wants1 == true);
/* critifcal section */
wants2 = false;
}
}

here, want1 and wants2 are shared variables, which are initialized to false.
which one of the following statements is true about the above construct?

a. it does not ensure mutual exclusion

b.  it does not ensure bounded waiting

c. it requireds that processes enter the critical section in strict alternation

d. it does not prevent deadlocks, but ensure mutual exclusion.

d is the correct answer. If they both enter the entry section they'll
set the variable then wait indefinitely for each other.

tip: see the things that you know about and check what happens.

question 

consider three concurrent processes p1, p2, and p3 as shown below, which access a shared variable D that
has been initialized to 100.

p1		p2		p3
--------------------------------------------
	.	.		.
	.	.		.
	.	.		.
D = D + 20	D = D - 50	D = D + 10
	.	.		.
	.	.		.

The processes are executed on a uni-processor system running a time-shared operating system. If the minimum and 
maximum possible values of D are the process have completed execution are X and Y respectively, then the value of
y-x is ___


1.80
2.130
3.50
4. none of these

1. is the correct answer.
130 is the maximum
50 is the minimum

130 - 50 = 80

question
--------

A counting semaphore was initialized to 10. then 6 P(wait) and 4V(signal) operations were completed on this semapohre 
the resuting value of the semaphore is.


a. 0
b. 8
c. 10
d. 12

10 - 6 + 4 = 8

b is the correct answer.


---------------------------------Quiz on synchronization--------------------------------------------
Q1
in __ only one process at a time is allowed into its critical section, among all processes that have
critical secgion for the same resource.

1. mutual exclusion
2. synchronization
3. deadlock
4. starvation 

1. mutual exclusion is the correct answer

Q2
Process synchronization can be done at the 
1. software level
2. hardware level
3. both software and hardware level
4. none of theses

3. is the correct answer

Q3
Mutual Exclusion can be provided by
1. binary semaphore
2. mutex locks
3. both binary semaphores and mutex locks
4. none of these

3. both binary semaphore and mutex locks.

Q4 
Peterson's solution is restricted to ____
processes that alternate execution between their critical se ctions
and remainder sections.

1. one
2. two
3. three
4. all of the above

2. is the correct answer


Q5
A solutiono to synchronization problem using test and set lock
1. satisfies mutual exclusion 
2. statisfies bounded waiting 
3. Does not satisfy mutual exclusion
4. None of the above

1. is the correct answer

Q6
A semaphore S is an integer value variable hat apart form initialization, is accessed only
through two standard atomic operations

1. exit() and exit()
2. exec() and signal()
3. wait() and exit()
4. wait() and signal()

4. is the correct answer

Q7 
Where two or more processes are waiting indefinitely for an event that can be caused only by
one of the waiting processes is called
1. mutex locked
2. deadlocked
3. spinlocked
4. none of the above

2 deadlocked

Q8 
Peterson's solution is a classic __ based solution to the critical section problem
1. software
2. hardware
3. software and hardware
4. none of the above

1. is the correct answer

Q9 
The bounded buffer problem is also known as 
1. readers-writers problem
2. dining philosphers problem
3. producer consumer problem
4. dining writers problem

3. is the correct answer

Q10 
In readers-writers problem multiple processes re permitted to concurrently acquire a reader-writer lock
in read mode, but only one process may acquire the lock for __ as 
exclusive access is required for __
1.reading, readerse
2. reading, writers
3. writing, writers
4. none of the above

3. is the correct answer.

Q11
In producer-consumer problem which of the following assumptions are true?

1. we assume that the pool consists of n buffers, each capable of holding one unit of data
2. the mutex semaphore provideds exclusion for access to the buffer pool and is initalized to the value 0
3. the mutex semaphore provideds mutual exclusion for access to the buffer an is initialized to the value 1
4. the empty and full semaphores count the number of empty and full buffers. the semaphore empty is initalized
to the value n; the semaphore full is initialized to the value 0

1. is the correct answer.

Q12
The solution to the dining-philosphers problem using monitors is 
1. deadlock-free
2. starvation-free
3. page-fault free
4. all of the above

1. is the correct answer

Q13
Which are the states in which a philospher can be in the dining-philosphers problem?
1. thinking
2. eating
3. hungry
4. all of the above

4. is the correct answer

Q14
All processes share a semaphore variable mutex, which is initialized to 1. each process muthst
execute wait bbefore entering the crtical section and
signal afterwards. suppose that a process interchanges teh order in which the wait()
and signal operation() on the semaphore mutext are executed, resulting
 in the following execution. signal(mutex) /*critical section */ wait(mutex) in this situation.

1. a deadlock will occur
2. processes will starve to enter their critical section
3. several processes maybe executing in their critical sections simulaneously
4. all of the above

3. is the correct answer.

Q15
A situation where several processe access and manipulate the same data concurrently and the outcome of the execution depends
on the paricular order in which the access takes place is called
1. Deadlock
2. Race condition
3. Spin lock
4. Page fault

2. is the correct answer.

-----------------------------------------------------Quiz on syncrhonization------------------------------------------
Q1 What are three properties of a good implementation of synchronization?
1. mutual exclusion
2. progress
3. bounded waiting
Q2 Can the OS know when a program is in a critical section?
no it can't we can only enforce safety through mutex locks
Q3 Describe Peterson's algorithm
Peterson's solution is an software implementation
of syncrhonization. it only works on 2 processes.
it is starvation free.
Q4 How does test and set work?
Test and set checks to see if the variable is the
expected value and if so swaps it. test and set
does not satisfy bounded waiting.
Q4 What is a semaphore? Which two operations does it support? are they atomic?
a semaphore is an integer variable that represents the amount of resources avaialable. it supports wait and signal. both operations must be atomic.
Q5 Describe how you would syncrhonize producer and consumer threads using a semaphore.
You would need a mutex lock and then a semaphore
that represents the amount of items produced in buffer and
another semaphore that represents the amount of
items empty in the buffer

Q5 Can deadlock occur with only one thread? why or why not?
yea it can by trying to acquring resource it already has

-------------------------------Quiz---------------------
Q1 what is synchonrization
Ensuring the orderly execution of cooperating
process to ensure data consistency
Q2 Producer-Consumer problem
classic syncrhonization problem where
Producer does not produce if no room to produce
and consumer does not consumer if there is no
item on the buffer.
Q3 Cooperating process
can affect or be affected by other executing processes
Q4 Race Condition
outcome of execution depends on order in
which access takes place
Q5 critical section
piece of program that requires mutual exclusion.
Q6 critical section problem
Design protocol that processes can use to cooperatate
ant not be executing in the critical section at the 
same time.
Q7 Entry section
section of code where process asks for permission
to enter critical section.
Q8 solution for critical section
-mutual exclusion
-progress
-bounded waiting
Q9 Mutual Exclusion
Only one process can be in its critical section at a given time.
Q10 progress
If no process is in its critical section and one
process wants to enter critical section, then the 
selection of the next process that enter critical section
cannot be postponed.
Q11 bounded waiting
bound must exist on the # of times other processes
are allowed to enter their critical section after
a process has made a requirest to enter its criticlhal
section and before that request is granted.
Q12 Premeptive kernel
allowed processes to be preempted while running in kernel mode. aka interrupted.
Q13 Nonpreemptive kernel
processes cannot be interrupted while in kernel mode. runs until exit kernel mode, blocks or voluntarily yields CPU.
Q14 Peterson solution
good algormithic description of critical section problem
at software level implementation
Q15 hardware solution
atomic hardware instructions
Q15 locking
protecting critical regions through locks
Q16 mutex
mutual exclusion locks
Q17 sempahore
synchronization tool that is more sophisticated than
mutex locks
Q18 Deadlock
two process waiting indefinitely for an event
that can be caused only by one of the waiting processes
Q19 starvation
Indefinite blocking, process in semaphore queue
may never be removed in which it is suspended.
Q20 classic synchronization problems
bounded-buffer problem
readers-writers problem
dining-philosphers problem
Q21 bounded buffer-problem
Same as the consumer-producer problem
Q22 readers-writers problem
allow multiple readers to read at a time but
only one writer can access at a time.
Q23 dining philosphers problem
5 philophopers with 10 chopsticks are either in 
thinking state or eating state. if in eating state
must use the two chopsticks adjacent to them to eat.
synchronization problem.
---------------------------------Quiz-----------------------
Q1 A situation where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which access takes place is called:
a. data consistency
b. race condition
c. aging
d. starvation

b is the correct answer

Q2 The segment of code in which the process may change
common variables, update tables, write into files is known as :
a. program
b. critical section
c. non-critical section
d. syncrhonizing

b is the correct answer.

Q3 The following conditions must be satisfied to solve the critical section problem:
a. aging
b. mutual exclusion
c. deadlock
d. progress
e. bounded waiting

b, d and e are the correct answers

Q4 Mutual exclusion implies that 
a. if a process is executing in its critical section, then no oter process must be executing in their critical section
b. if a process is executing in its critical section, then other processes must be executing in their critical sections.
c. if a process is executing in its critical section, then all the resources of the system must be blocked until it finished execution
d. none of these

a is the correct answer.

Q5 Bounded waiting implies that there exists a bound on the number of times a process is allowed to enter its critical section:
a. after a process has made a request to enter its critical section and before the request is granted
b. when another process is in its critical section
c. before a processs has made a request to enter its critical section
d. none of these

a is the correct answer.

Q6 An un-interruptible unit is known as:
a. single
b. atomic
c. static
d. none of these

b is the correct answer.

Q7 The TestAndSet instruction is executed:
a. after a particular process
b. periodically
c. atomically
d. none of these

c is the correct answer.

Q8 semaphore is a/an __ to solve the critical section problem
a. hardware for a system
b. special program for a system
c. integer variable
d. none of these

c. is the correct answer

Q9 the two atomic operations permissible on semaphores are
a. wait
b. stop
c. hold
d. signal

a and d are the correct answers.

Q10 spinlocks are:
a. CPU cycles wasting locks over critical sections of programs
b. locks that avoid time wastage in contex switches
c. locks that work better on multi-processor system
d. all of these

d is the correct answer

Q11 The main disadvantage of spinlocks is that:
a. they are not sufficient for many process
b. they require busy waiting
c. they are unreliable sometimes
d. they are too complex for programmers

b. is the correct answer 

Q12 If the semaphore value is negative:
a. its magnitude is the number of process waiting on that semaphore
b.  it is invalid
c. no operation can be further performed on it until the signal operation is performed on it
d. none of these

a. is the correct answer

Q13 the code that changes the value of the semapohre is 
1. remainder section code
2. non-critical section code
3. critical section code
4. none of these

3 is the correct answer

Q14 what will happen if a non-recursive mutex is locked more than once?

b is the correct answer

A semaphore:
a. is a binary mutex
b. must be accessed from only one process
c. can be accessed from multiple processes
d. none of these

c is the correct answer

Q15 the two kinds of semaphores are:
a. mutex
b. binary
c. counting
d. decimal

b and c are the correct answer.

Q16 the bounded buffer problem is also know as:
a. readers-writers problem
b. dining-philospher problem
c. producer-consumer problem
d. none of these

c is the correct answer

Q17 In the bounded buffer problem, there are the
empty and full semaphores that:
a. count the number of empty and full buffers
b. count the number of empty and full memory spaces
c. count the number of empty and full queues
d. none of these

a is the correct answer

Q18 To ensure difficulties do not arise in
the readers-writers problem, __ are given exclusive
access to the shared object
a. readers
b. writers
c. none of these

b. is the correct answer.


Q19 The monitor construct ensure that:
a. only one process can be active at a time within the monitor
b. n number of processes can be active at a time within the monitor
c. the queue has only one process in it at a time
d. all of these

a is the correct answer.

Q20 The operations that can be invoked on a condition variable are:
a. wait
b. hold
c. signal
d. continus

a and c are the correct answer


---------------------------------------------------------------------------------------------------

    while(true){
        /* produce an item in next produced */
        while(count == BUFFER_SIZE);

        buffer[in] = next_produced;
        in = (in + 1) %  BUFFER_SIZE:
        count++;
}


    while(true){
        while(count == 0);
        next_consumed = buffer[out];
        out = (out + 1) % BUFFER_SIZE;
        count--;
}


possible implementation of count++

register_1 = count;
register_1 = register_1 + 1;
count = register_1;

possible implementation of count--

register_2 = count;
register_2 = register_2 - 1;
counter = register_2;

we can arrive at the incorrect state because we allow both
processes to manipulate the variable count concurrently.

race condition: outcome of the execution depend
on the particular order in which the access takes place.

in order to prevent a race condition we have to make
sure that only one process at a time can access the 
count variable.

criticial section: process is accessing or updating
data that is shared with at least one other process.

we want so that no other process is allowed to execute in
the critical section while another one is executing.

critical section problem: design a protocol that
the processes can use to synchronize their activitity so
as to cooperatively share data.

each process requires permission to enter critical section.

entry section: section of code in the critical section
that implements the permission for a process to enter

solution to critical-section problem must satistfy three requirements.

1. Mutual exclusion: If process pi is executing in its critical section, then no other proesses can be executing in their critical sections.

2. progress: if no process is executing in its critical section and some processes wish to enter their critical section. then only entries not executing remainder section can decide which will enter its critical section.

3. bounded waiting: bound on number of tiems other processes
are allowed to enter their critical section after a process
has made a request to enter its critical section.

while (true){
    entry section
            critical section
    exit section
            remainder section
}


peterson solution:

int turn;
boolean flag[2];
while(true){
        flag[i] = true;
        turn = j;
        while(flag[j] && turn == j);
        /** critical section */
        flag[i] = false;
        /*remainder section */
}

peterson solution does not work on mult-threaded applications.

software-based solutions are not guaranteed to work on modern
computer architectures.

memory model:

1. strongly ordered: when a memory modification on one processor is immediately visible to all other processors.

2. weakly ordered: where modifications to meomry on one processor may not be immediately visible to other processors

memory modifications are visible to threads running on 
other processors: memory barriers.

while(!flag)
    memory_barrier();
print x;

x = 100;
memory_barrier();
flag = true;

memory barriers are considered very low-level operations.

hardware instructions

atomically: one uninterruptible unit.

boolean test_and_set(boolean* target){
    boolean rv = *target;
    *target = true;
    return rv;
}

do {
    while(test_and_set(&lock));
    /* do nothing */

    /* critical section */

    lock = false;

    /* remainder section */

} while(true)

int compare_and_swap(int *value, int expected, int new_value){
    int temp = *value;
    if(*value == expected)
        *value = new_value;
    return temp;
}

while(true){
        while(compare_and_swap(&lock, 0, 1) != 0);
        /* critical section */
        lock = 0;
        /* remainder section */
}

this algorithm satisfies the mutual exclusion property but
does not satisfy the bounded waiting property.

the code segment below satisfies all the critical section requirements

while(true){
    waiting[i] = true;
    key = 1;
    while(waiting[i] && key == 1)
        key = compare_and_swap(&lock, 0, 1);
    waiting[i] = false;
    
    /* critical section */

    j = (i + 1) % n;
    while((j != i) && !waiting[j])
        j = (j + 1) % n;

    if(j == i)
        lock = 0;
    else
        waiting[j] = false;
    /** remainder section */
}

atomic variable is used to ensure mutual exclusion
in situations where there may be a data race on 
a single variable while it is being updated as when a counter
is incremented. 

increment(&sequence);

void increment(atomic_int *v){ 
    int temp;
    do {
        temp = *v;
    } while(temp != compare_and_swap(v, temp, temp + 1);
}

still not a viable solution to the consumer producer problem.

mutex lock

mutex is short for mutual exclusion

mutex lock has a boolean variable available whose
value indicates if the lock is available or not.
If the lock is available, a call to acquire() succeeds.
and the lock is then considered unavailable. A process
that attempt to acqurie an unavailable lock is blocked
until the lock is released.

while (true){
    acquire lock
        critical section
    release lock
        remainder section
}

// the definition of the acquire function

acquire(){
    while(!available);
    available = false;
}

// the definition of the release function
release(){available = true;}

calls to either acquire() or release() must be performed atomically.

busy waiting: while a process is in its critical section,
any other process that tries to enter its critical section
must loop continously in the call to acquire().

this type of mutex lock is also called a spin-lock.

advantage: no context switching is needed.
disadvantage: processes have to loop continously while
waiting to enter the critical section.

mutex locks are considered the simplest of synchronization tools

semaphores: integer variable that is only accessible
through the wait and signal system calls..

wait(s){
    while(s <= 0);
    s--;
}


signal(s){s++;}

when one process
modifies the semaphore value, no other process can simultaneously modify
that same semaphore value.

counting semaphore: can range over an unrestricted domain.

binary semaphores: can range only between 0 and 1.

binary semaphores behave similary to mutex locks.

systems that do not have mutex locks can use binary semaphors
for mutual exclusion.

counting semaphores are used to control access to a given
resource consiting of a finite number of instances.

the semaphore is initialized to the number of resources available.

when a process releases a resource, it performs a signal() operation.

each process that wishes to use a resource performs a wait()
opeartion on the semaphore.

when the count for the semaphore goes to 0, all resources are being used. when a process is done with a resource it 
performs a signal.
After that, processes that wish to use a resource will block until the count becomes greater than 0.

s1;
signal(synch);
wait(synch);
s2;


the current implementation suffers from busy waiting.

we modify the definition so that we no longer are busy waiting.

typedef struct{
    int value;
    struct process* list;
} semaphore;

wait(semaphore *s){
        s->value--;
        if(s->value < 0){
            add this process to s->list;
            sleep();
        }
}


signal(semaphore *s){
    s->value++;
    if(s->value <= 0){
            remove a process P from s->list;
            wakeup(P);
    }
}

----------------------Monitors---------------------------------

Timing errors can still occur when mutex locks or semaphores are used.

A monitor type is an ADT that includes a set of programmer-defined operations that are provided with mutual exclusion
within the monitor. 

errors can be caused by a programmer.

// problem 1
signal(mutex);

/*
    critical section

*/

wait(mutex);

// problem 2

wait(mutex);

/*
    critical section
*/

wait(mutex);

// problem 3
one of the calls of wait or signal is ommitted.

monitors are high-level constructs of synchronization tools.

Monitor is an abstract data-type

by themselves are not sufficient enough to handle sychronization neeeds constructs like conditioning.

condition x, y;

the only operation that can be invoked on a condition variable
are wait() and signal(). The operation

    x.wait();

mean that the process invoking this operation is suspended until
after another process invokes

    x.signal();


x.signal() is called by a process p while a process q is suspended.

2 possibilities exist

1. signal and wait

2. signal and continue

many programming languages have incorporated monitors.

any function is replaced by

wait(mutex);
/*
    body of F
*/
if (next_count > 0)
    signal(next);
else
    signal(next);

x_count++;
if (next_count > 0)
    signal(next);
else
    signal(mutex);
wait(x_sem);
x_count--;


if (x_count > 0){
    next_count++
    signal(x_sem);
    wait(next);
    next_count--;
}
-------------implementing a monitor using semaphores------------
use a binary semaphore for each monitor.

using the signal and wait approach

wait(mutex);
/*
*/

if(next_count > 0)
    signal(next);
else signal(mutex);



---------------------------------synchronization examples-----------












-------------------------------Operating System Interfaces -------------------
1. User interface
    1. command line interface
    2. Graphical User Interface

2. Program execution    

3. I/O operations

keyboard
mouse
monitor
printer
speaker
other input
other input
other output

4. File System Manipulation
access restriction is controlled by operating systems

5. Communications

processes that communicate with each other so their execution
can be done in an effective way.

6. Error detection

important service provided by the operating system

7. Resource Allocation

A process keeps waiting for a resource but never gets it
Process gets resource but never releases it

8. Accounting

Which user uses and how much resources and of what type
keeping usage statistics.

9. Protection and Security

Processes are executing at the same time, a process should
not be able to interfere with other processes operations.


----------------------------------User Operating System Interface-----------------------------------------------------

There are two fundamental approaches for users to interface with
the operating system.

1. Provide a command-line interface or command interpreter that allows users to directly enter commands that are to be performed by the operating system.

2. Allow the user to interface with the operating system via a graphical user interface or GUI.

Some operating systems include the command interpreter in the kernel

others such as windows XP and UNIX, treat the command interpreter as a special program.

On systems with multiple interpreter to choose from the interpreters are know as shells

bourne-shell
c shell
bourne again shell
korn shell

System Calls

System Calls provide a interface to the services made available
by an operating system. 

User mode and kernel mode

two modes in which a program can execute.

user mode does not have direct access to memory or hardward
or such resources

kernel mode is privileged mode.

kernel mode programs that crash will crash entire system.

user mode program crash the whole system will not crash.

user mode is more safe so more program exectute in usermode.

switching from kernel to user and vice verse is known as mode
shifting.

system calls provide an interface to the services made available
by an operating system.

System calls is the programmatic way in which a computer program
requests a service from the kernel of the operating system.

example we want to read data from one file to another

Acquire input filename
write prompt to screen
accept input
Acquire Output filename
Write prompt to screen
Accept Input
Open Input File
If file doesn't exist, ABORT
CREATE Output file
If file exists, ABORT
write completion message to screen
terminate normally

had to use lots of system calls even for very simple tasks.
system calls are always being executed.

These calls are generally avaiable on routines written in
C and C++.

-----------------Types of Systems Calls----------------------

1. Process Control
2. File Manipulation
3. Device Management
4. Information Maintenance
5. Communications

1. Process Control

-end, abort
-load, execute
-create process, terminate process
-get process attributes, set process attributes
-wait for time
-wait even, signal event
-allocate and free memory

2. File Manipulation
-create file, delete file
-open, close
-read, write, reposition
-get file attributes, set file attributes

3. Device Manipulation
-Request device, Release Device
-Read, Write, Reposition
-get device attributes, set device attributes
-logically attach or detach devices

4. Information Maintenance
-get time or date, set time or date
-get system data, set system data
-get process, file, or device attributes
-set process, file, or device attributes

5. Communication
-create, delete communcation connection
-send, receive messages
-transfer status information
-attach or detach remote devices

These are the five major categories we can grouped system calls

--------------System Programs----------------------------------
An important aspect of a modern system is the collection of 
system programs.

-System programs provide a convenient environment for
program development and execution

File Management
-create
-delete
-copy
-rename
-print
-dump
-list

Status information
-date, time
-amount of available memory or disk space
-Number of users
-Detailed performance
-Logging, and debugging information

File Modification modifiying inner content of files

Programming-language support
-compilers
-assembler
-debuggers
-interpreters

program-loading and execution
-absolute loaders
-relocatable loaders
-linkage editors
-overlay editors

debugging systems for either higher-level langauges or machine
language are needed as well.

Communications

-creating virtual connections among processes, users, and computer systems
-allowing users to send message to another's screen
-to browse webpages
-to send electronic-mail message
-to log in remotely or to transfer files from one machine to another.

In addition to system programs, most operating system 
have programs to solve common problems.

--------------Operating System Design & Implementation----------
1st Problem: Defining Goals and specifications
2nd Problem: Type of system

Beyond this highest design level, the requirements may be
much harder to specify.


requirements
-user goals:This system should easy to use, convienent and safe
-system goals: Easy to design, implement, operate

Mechanisms and policies:

Mechanisms determine how to do something

policies what will be done.

One important is the separation of policy from mechanism.

separation of policies and mechanism will make the system
good and flexible.

First operating systems were written in assembly language.

Now most commonly written in higher-level languages 
such as C or C++.

advantages of writing in higher-level language
-the code can be written much faster
-it is most compact
-it is easier to understand and debug
-it is easier to support

-Operating systems vary greatly in their makeup internally

- COMMONALITIES:
    -multiprogramming
    A single user cannot, in general, keep either the CPU or the I/O devices busy at all times
    - multi-programming increases CPU utilization by organizing
    jobs so taht the CPU always has on to execute
    - Job contain code and data
    - All jobs that are to be executed contains job pools
    - cannot load all jobs at once become memory is limited.
    - If we don't have job programming everytime we run a job
      we have to wait until the job is finished completely
      before another job can use the CPU.
    - Multi-programming: One Job uses CPU then switches to IO
      then job 2 uses CPU until Job 1 is done with IO
    - For a operating system to be effecient it should be
      able to ultilize multi-programming
    - Time-sharing(Multi-tasking system)
        -CPU executes multiple-jobs by switching among them
        -Switches occurs so frequently that the users can interact while it is running
        -Time-sharing requires an interactive computer system,
         which provides direct communcication between the user
         and the system
        - A time-shared system allows many users to share the computer
         simultanelously
        - while their is time-gap between system and the user that
          the computer can serve other users.
        -Uses CPU scheduling and muli-programming to provide
         each user with a small portion of a time-share computer
        -Each user has at least on separate program in memory
        -A program loaded in memory and executing is called a "Process"
        
-------------------Structures of Operating Systems-------------
Simple Structure

Application Program
|
|
|
Resident Systems Programs
|                   
|
Device Drivers
|
|
ROM BIOS device drivers

All things above can access elements in the bottom

Monolithic Structure

            
The users


Shells and commands
compilers and interpreters
system libraries

kernel

If you want to add something to this system it would be
difficult to add to it.

Layers Structure

layer can only use layer below another layer.
designing this layered structure is difficult.

system can be slow.

Microkernels

we have microkernel.
implement non-essential fo kernel as programs.

micro-kernel can have performance decrease.
communication can lead to performance decrease.

Modules:

current best way to create modular kernel in objected-oriented
programming.

Each kernel system has a protected interface.Any module can
call any other module. more flexible than layered structure.

One of the best structuring of OS we can have.

-------------------------Virtual Machines-----------------------

-abstract the hardware of a single computer into different 
execution environments. create the illusion of many computers.

-Non-virtual machine has to do one process at a time

-virtual machine can split up so that each process has 
it's own illusionary computer.

implementation

virtual-machine software - runs in kernel mode
virtual-machine itself -   runs in user mode

A virtual has two modes just like a physical machine

1. A virtual user mode
2. A virtual kernel mode

both of which run in a physical user mode

---------------------Operating System Generation & System Boot-----------------

-Design code, and implement an operating system specificallly
for one machine at one site

-Operating systems are designed to run on any of a class
of machines at a variety of peripheral configurations

-The system must then be configured or generated for each
specific computer site, is process sometimes know as 
system generation is used for this

-The following knids of information must be determined by the SYSGEN Program
    - What CPU is to be used?
    - How much memory is available
    - What devices are available
    - What operating-systems options are desireds

-System Boot
    - The procedure of starting a computer by loading the kernel is known as booting the system
    - On most computer systems, a small piece of code known as bootstrap program or bootstrap loader locates the kernel
    - This program is in the form of ROM. ROM 
      needs no initialization and cannot be infected by a computer virus

Firmware - Another type of read-only memory
If you want to change something in the ROM you have to change
everyone on the ROM chip.

EPROM-erasable programmable read-only memory

When the full bootstrap program has been loaded, it can
traverse the file system to find the operating system
kernel, load it into memory, and start its execution.
It is only at this point that the system is said to be
running.

---------------------Process Management-----------------------

- Process can be though of as a program in execution.

- A thread is the unit of execution within a process. A process
  can have anywhere from just one thread to many threads.

A process can have many threads.


--------------------------------Process States------------------

- As a process executes, it changes states

- The state of a process is defined in part by the current activity of that processs

each process may be in the following states

new -> the process is being created

running -> instructions are being executed

waiting -> the process is waiting for some event to occur

ready ->   the process is waiting to be assigned to a processor

terminated ->   the process has finished execution

-------------------------Process Control Block------------------
Each process is represeneted in the operating system by 
a process control block. also called a task control block

process id-every process is identified by a unique id
process state-the particular state a process is in a particular moment
process number-
process counter-address of next line of instruction to be executed
registers-Tells us the registers that are being used in a given process
memory limits
list of open files

I/O status information: Input Output devices assigned to a process

process control block is used to represent a particular process


-----------------------Process Scheduling-----------------------
-The objective of multi-programming is to have some process
running at all times, to maximize CPU utilization.

-The objective of time-sharing is to switch the CPU among processess so frequently that users cn interact with each program with each program while it is running.

-To meet these objectives, the process scheduler selects on
available process for program execution on the CPU.

-For a single processor system, there will never be more than one running process

-If there are more processes, the rest will have to wait until
the CPU is free and can be rescheduled.

JOB QUEUE-As process enter the system, they are put into a job queue,
which consists of all processes in the system.

READY QUEUE-The processes that are residing in main memory
and are ready and waiting to execute are kept on a list called the ready queue.

example
From job queue goes to ready queue. completes execution
and terminates.

from job queues goes to ready queue. executing but then
a more import process comes so the job is swapped out into
partially executed process then ready queue.

-------------------------------Context Switch-------------------
-Interrupts cause the operating system to change a CPU from
its current task to run a kernel routine

-Such operations happen frequently on general-purpose systems

-When an interrupt occurs, the system needs to save the curent context of the process currenlty running on the CPU so that it
can restore the context when its processing is done, essentially
suspending the process and then resuming it.

-The context is represented in the PCB of the process.

-At home reading a book.the process is reading a book. suddenly 
mother comes and asks and you to do something. you are being
interuptted reading a book to help your mother. put book mark
on book and then help your mother with the task. save the current process of what you were doing.

-Switching the CPU to another process requires performing
a state save of the current process and a state restore
of a different process.

This task is known as a context switch. ssave the
state of the process and then restore the state of the 
process.

-Context-switch time is pure overhead, because the system does no useful work while switching

-It speed varies from machine to machine

-typical speeds are a few milliseconds

------------------------Operations on Process-------------------
-A process may create several new processes, via a create-process system call, during the course of execution
-The creating process is called a parent process, and the new processes are called the children of the process
-Each of these new processes may in turn create other process, forming a tree of processes
-from shell you can issue commands like ls and cat.
-When a process creates a new process, two possibilities exist in terms of execution
1.The parent continues to execute concurrently with its children
2. the parent waits until some or all of its children have terminated


There are also two possibilities in terms of te addresss space of the new process:
1.the child proces is a duplicate of the parent process
2. the child process has new program loaded into it


---------------------------Process Termination------------------
-A process terminates when it finishes executing its final
statement and asks the operationg system to delete it
by using the exit() system call.
-Use the exit() call a process ask the OS to terminate the process
-At the point the process may returna status value to its parent process
-All the resources of the process-including physical and
virtual memory, open files and I/O buffers are deallocated
by the operating system.
-Termination can occur in other circumstances as well
-A process can cause the termination of another process vian
an appropiate system call.
-Usually such a system call can be invoked only by the paren to the process that is to be terminated
-Otherwise, users could arbitrarily kill each other's job.

A parent may terminate the execution of one of its children
for a variety of reasons, such as these:

-The child has exceed its usage of some of the resources that
it has been allocated.

-The task assigned to the child is no longer required

-The parent is exiting, and the operating system does not allow a child to continue if its parent terminates

-----------------------------Interprocess Communication---------
-Processses executing concurrently in the operating system
may be either indepenent processes or cooperating processes
-Independent Processes-They cannot affect or be affected by the other processes executing in the system
-Cooperating Processes-They can affect or be affected b teh other processes executing in the system.
-There are several reaons for providing an environment that allows process cooperation

-Information Sharing
-Computation speedup
-Modularity-design system so that we have separate modules
-Convenience-allows users to do multiple tasks at the same time
-Cooperating processes require an itnerprocess communication
mechanism that will allow them to exchange data and information.
-There are two fundamental models of interprocess communication.
-shared memory
-message passing


-In the shared-memory model, a region of memory that is shared by cooperating processes is established
-Processes can then exchange information by reading and writing data to the shared region.
-When one process wants to communicate with another process it will write into shared memory region
-the other process can read that data

--------------------------------Shared Memory Systems-----------
-Interprocess communication using shared memory requires
communicating processes to establish a region of shared memory
-typically, a shared-memory region resided in teh address space
of the process creating the shared-memory segment
-other processes that wish to communicate using this shared-memory segment must attach it to their address space
-normally, the operating system tries to prevent one process from access another process's memory
-shared memory requires that two or more processes agree to 
remove this restriction
-A producer process produces information that is consumed
by consumer process.
-For example, a compiler may produce assembly code, which
is consumed an assembler. the assembler, in turn, may produce
object modules,which are consumed by the loader.
-they have to work concurrently so that consumer and produce
consume in equilibrium
-one solution is to use shared-memory
-buffer of items that are filled by producer and consumed
by consumer.
-The producer and consumer must be syncronized so that the
consumer does not try to consume what has not been produced


two kinds of buffers
-unbounded buffers-no limit ont the size of the buffer. consumer
may have to wait for new items, but the produces can always produce new items
-bounded buffers=assumes a fixed buffer size, in this 
case, the conusmer must wait if the buffer is empty.
and the producer must wait if the buffer is full.


---------------------------Threads------------------------------
A program under execution is a process
each process can have a number of threads
basic unit of CPU utilizaiton

thread is composed of
a thread ID
a program counter
a register set
a stack

It shares with other threads belong to the same process
its code section, data section, and other operating system
resources such as open files and signals.

A traditional / heavyweight process has a single thread of control

If a process has multiple threads of control, it can perform more than one task at a time..

single-threaded process

--------------------
| CODE DATE FILE   |
| registers  STACK |
|                  |
|                  |
|   THREAD         |
|                  |
|                  |
--------------------

As shown in the figure above a program that is a single-threaded
process can only have a single thread of control. There is
only one program counter and one sequence of instructions
that can be carried out at any given time.


multi-threaded process



--------------------
| CODE DATE FILE   |
| r1   r2   r3     |
| s1   s2   s3     |
|                  |
| t1   t2   t3     |
|                  |
|                  |
--------------------

as shown above each thread has their own program counter,
their own registers and stack but they share the code
data and files.

You can look at a process to see the threads associated with
it.

each thread may be doing different tasks

one thread is dedicated to viewing webpage
another page is spent downloading.

the benefits of multi-threading programming can be broken down into four major categories

responsiveness -    multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a length operationg, thereby moving responsiveness to the user.

resource sharing - by default, threads share the memroy and
the resources of the process to which they belong. the benefit
of sharing code and data is that it allows an application
to have several different threads of activity within the same
address space

allocating memory and resources for process creation is costly.
because threads ahre resoucrse of the process to which they
belong, it is more economical.

utlization of multi-process architecture.
multi-threading can be greatlu increased in a mult-processor

Multi-threading models and Hyper-threading

1. User-threads - Supported above the kernel and are managed without kernel support
2. Kernel-threads - Supported and managed directly by the operating system

Ultimately, there must exist a relationship between user threads and kernel threads.

Many-to-One model

kernel thread has many user threads.

maps many user-level threads to one kernel thread

thread management is one by the thread library in user space, so it is effecient.

the entire process will block if a thread makes a blocking system call

because only one thread can accesss the kernel at a time, multiple threads are unable to run in parallel on multi-processor

one-to-one model

one user thread is mapped to one kernel thread

maps each user thread to a kernel thread
more concurrency
allows multi-threads to run in parallel on mult-processors

-creating user thread requires corresponding kernel thread
-restrict the number of threads supported by system.

Many-to-Many model

many user threads are mapped to many user threads

many user threads to smaller many threads
developers can create as many user threads as necessary
and the corresponding kernel. threads can run in parallel
on multiprocessor.

Also when a thread performs a blocking system calls, the kernel
can schedule another thread for execution

-----------------fork() and exec() system calls-----------------

fork(): The fork() system call is used to craete a separate, duplicate process.

create exact replica of that process. separate duplicate
process will be create.a different process.
duplicate process will be called a child process.

exec(): when an exec() system call is invoked, the program
specified in the parameter to exec() will replace the entire
process - including all threads.

replace a process with another. they will have the same process
id because we are replacing a process.


---------------------------threading issues-------------

duplicate all threads or duplicate thread taht invoked
the fork() system call. If a thread invokes
the exec() system call, the program specified in 
the parameter to exec() will replace the entire
process---including all the threads.

which of the two version of fork() to use depends on
the applications.

if exec() is called immediately forking

then duplicating all threads is unnecessary,
as the program specified in the parameters to exec()
will replace the process. In this instance, duplicating
only the calling thread is appropriate.

If the separate process does not call exec() after forking
then the separate process should duplicate all threads.

this is how we use fork depending on the different 
scenarios that we could have.

------------------Thread Cancellation--------------------

Thread cancellation is the task of termination a thread before it has been completed. Before a thread could complete its execution it was terminated.

example of thread cancellation 
If multiple threads are concurrently searching
through a database and one thread returns the result,
the remaining threads might be canceled.

When a user presses a button on a web browser
that stops a web page from loading any further,
all threads loading the page are canceled.

A thread that is to be canceled is referred to as a 
the target thread.

Cancellation of a target thread may occur in 
two different scenarios 

1. Asychonronous cancellation: One thread immediately
terminates the target thread.

the thread has no power, it is cancelled by 
some other thread.

2. Deferred cancellation: The target thread 
peridoically check whether it should be terminate,
allowing it an opportunity to terminate itself
in an orderly fashion.

the thread has some power, it will not be directly
cancelled by some other thread.

Where the difficulty with cancellation lies

1. Resources have been allocated to a cancelled thread

2. A thread is cancelled while in the midst of
updating it is sharing with other threads.

Often, the OS will reclaim system resources rom
a canceled thread but will not reclaim all resources.

therefore, canceling a thread asynchronously
may not free a necessary system-wide resource.

deferred cancellation is safer. The target thread
has checked itself and it is in a position to 
be cancelled. it will not cancel itself until it
is at a point where it can canceled safely.

deferred cancellation is the preferred method.

-----------------Thread review----------------------------
A thread is like a worker in a toy shop
---is an active entity
---executing unit of toy order
---works simultaneously with others
---many workers completing toy orders
---requires coordination
---sharing of tools, parts, work stations


---is an active enity
----executing unit of a process
----works simultaneously with others
----many threads executing
----requires coordination
----sharing of a I/O devices, CPU, Memory

Why are threads useful?

Each thread execute same code for different subset of input
matrix. by parallization we can achieve speedup. different
thread can work on different parts of the program. 

parallelizzation -> speedup
specilizaton -> hot cache
effecieny -> cheaper IPC

benefits to applications and OS

multi-thread kernels
thread working on behalf of apps
os-level services

----------------------------------------dead locks--------------


----------------System Models---------------------
Computer System has a finite number of resources
to be distributed among competing processes.
The resources may be partitioned into several
types where they also have a resource count.

EX: 
CPU: 2
PRINTER: 1
NETWORK: 2


-------normal mode of operation for a process-------
1. Request. The process requests the the resource. If the
request cannot be granted immediately the process
waits until it can acquire the resource.

2. Use. the thread can operated on the resource.

3. Release. The thread releases the resource.

------Deadlock in multi-threaded applications-------

	pthread_mutex_t first_mutex;
	pthread_mutex_t second_mutex;

	pthread_mutex_init(&first_mutex, NULL);
	pthread_mutex_init(&second_mutex, NULL);

/* thread_one runs in this function */
void* do_work_one(void * param){
	pthread_mutex_lock(&first_mutex);
	pthread_mutex_lock(&second_mutex);
	/*
		Do some work
	*/
	pthread_mutex_lock(&second_mutex);
	pthread_mutex_unlock(&first_mutex);

	pthread_exit(0);
}

/* thread_two runs in this function */
void* do_work_two(void *param){
	pthread_mutex_lock(&second_mutex);
	pthread_mutex_lock(&first_mutex);
	/*
		Do some work
	*/
	pthread_mutex_unlock(&first_mutex);
	pthread_mutex_unlock(&second_mutex);
	pthread_exit(0);
}

A deadlock will occur if the first thread
acquires the first mutex and the second thread
immediately acquires the second mutex, this will
result in both threads being indefinitely stalled
with no progress.

----Livelock----
liveness: a concurrent application's ability
to execute a in a timely manner.


another form of liveness failure.
similar to deadlock but occurs because threads
continously tries an actions that fails.
livelock typically occur when threads try failing
operations at the same time. It is generally
remedied by having each thread retry the failing
operation at random times.
----Deadlock Characterization----
four condition must hold simultaneously in a system
for there to be deadlock.

1. Mutual Exclusion. At least one resource must be held in a non-shareable mode. If another thread request a non-sharable resource another thread then it must wait until the resource has been released.

2. Hold and wait. A thread must be holding at least one resource and waiting for a resource hold by
some other threads.

3. No preemption. Resources cannot be preempted.
resource can only be volunitary be released by a 
thread after it completes it execution.

4. Circular wait.

A set {t0, t1, ..., tn} of waiting threads must exist
such that t0 is waiting for a resource by t1, t1 is waiting
for a resource by t2. ...,tn is waiting for a resource
for resource held by t0.

note that the conditions are not completely independent
but we consider them independent for simpler analysis.

----Resource-Allocation Graph----
Deadlocks can be represented as DAGS, which are
characterize system resource-allocation graph.
This graph consists of a set of vertices V and
a set of Edges E. 


T = {T1, T2, ..., Tn}, the set consisting of all the active threads in the system.

R = {R1, R2, ..., Rm}, the set consisting of all the resources types in the system.

edge
	T_i -> R_j signified that thread T_i

has requested an instance of resource type R_j and is currently waiting for that resource.

edge
	R_j -> T_i signifies that an instance of type
R_j has been allocated to thread T_i. A directed edge
T_i -> R_j is called a request edge. A directed
edge R_j -> T_i is called an assignment edge.


example jk
The sets T, R, and E:

	T = {T1, T2, T3}
	R = {R1, R2, R3, R4}


	E = {T1 -> R1, T2 -> R3, R1 -> T2, R2 -> T1, R3 -> T3}

Resource instances
	one instance of type R1
	two instances of type R2
	one instance of type R3
	three instances of type R4

If the RAG has no cycle then it has no deadlock.
But if a RAG does have a cycle then a deadlock 
may or may not exist. 

If each resource type has only one instance and there
is a cycle in the graph then there is a cycle.
In this case a cycle is both a sufficient and
necessary condition for the existence of a deadlock.

If each resource type has several instances,
then a cycle does not necessarily imply that 
a deadlock has occurred. In this case, a cycle
in the graph is a necessary but not sufficient condition
for the existence of a deadlock.

----Methods for handling deadlocks

Three ways to deal with the deadlock problem
1. Ignore the problem altogether and pretend it does not exist(ignorance)
2. Use a protocol to prevent or avoid deadlocks. ensure the system never enters a deadlock state.(prevention)
3. Allow the system to enter a deadlock state but we try to detect and recover from deadlocks.(avoidance)

Deadlock prevention provides a set methods
to ensure that at least one of the necessary
conditions cannot hold.

Deadlock avoidance requires that the operating 
system be given additional information in 
advance concerning what resources a thread will
request and use during its lifetime.

If a system does not employ a deadlock avoidance algorithm then a deadlock situation may arise. In this environment can provide an algorithm that examines the state of the system to determine where the deadlock has occurred and an algorithm to recover from the deadlock.

The final alternative if not employing deadlock avoidance or deadlock recovery algorithms is to simply ignore the problem. This will cause the system performance to deteriorate and eventually will need to be restarted manually.
This is used in most operating system because
deadlock rarely happen and the extra expense to
recover from or keep avoid them are not worth it.

------Deadlock prevention

For a deadlock to occur, each of the four necessary
conditions must simultaneously hold

1. Mutual-exclusion 	2. Hold & Wait

3. No preemption	4. Circular wait

	By ensuring that at least one of these
	conditions do not hold we can can
	prevent deadlocks.

8.5.1 Mutual Exclusion(one or more than resource are non-sharable)

The mutual-exclusion condition must hold.
Cannot prevent deadlocks by denying the mutual-
exclusion because some resources are
non-sharable inherently.

example: a printer cannot be simultaneously
be shared by several processes.

Shareable resources, in contrast, do not require mutual exclusion, and thus could not 
be involved in deadlock.

example: read-only files. If several processes try to access a read-only file, they can all simultaeously
access the file, a process should never need to wait for a shareable resource.

some resources inherently not shareable.
It is then not possible for us to avoid mutual-exclusion. 
It is out of the question,
must be there.

8.5.2 Hold and Wait(process is holding at least one resource and waiting for more resources)

To ensure that the hold & wait condition never occurs in the system, we must guarantee that, whenever process
request a resource it is not holding a resource.

1st protocol to prevent hold and wait condition:
process must request all resources it needs before its execution and then must
have all the resources allocated to it before it begins its execution resource.


2nd protocol to prevent hold and wait condition:
Allows a process to request a resource when it has none.
It must relinquesh it resources before it can for more. Mut 
also then wait for both it roueces and the requested resources beofre it can start again.

	both these two protocols have
	two main disadvantages
	-----------------------------
	1. resource utilization is low
	2. starvation is possible

examples:
1st protocol may have a resource but not be using it

2nd protcol when release resource it must wait to
get it back and others as well. could wait indefinitely.

8.5.3 No Preemption(A resource cannot be taken from a process unless the process releases the resource)

preemption should be there.

If a process is hodling resources and must wait for another process to get desired resource then all its current resource are preempted.

alternatively, if process requests some resource, we first check whether they are avaiable, if they are we allocate them, if no then we check whether they are holed by some other process that is waitinf ro additional resources, if so we prempted the desired resources from
the waiting process and allocate them for the requesting process

generally applicable to resources whose state can be easily stored and restored later.

----Circular Wait(A set of processes are waiting for each other in circular form)

p0->p1->p2->....->pn->pn

Three options above are impractical.

Way to ensure Circular wait does not hold is 
to impose a total ordering of all resource types and
to require that each thread requests resources in an increasing order of enumeration.

F(first_mutex) = 1
F(second_mutex) = 2

A thread request a resource R_i after that the 
thread can only request resources only iff
F(R_j) > F(R_i).

example 2.
P1 has a resource denoted r5. If p1 requests
r4 or r2 its request will not be granted,
P1 can only be granted resources greater
than 5.

An alternative strategy to employ is 
require that a thread requesting an instance of 
resource R_j must have released any resources
R_i such that F(R_i) >= F(R_j).

Even though this method is the most practical to
avoid deadlocks it is still up to the application
developer to follow the ordering.

----Deadlock Avoidance
Deadlock prveention algorithms prevent deadlocks
by limiting how requests can be made. The side
effects of preventing deadlocks by this method are
low-device utilization and reduced system throughput.

Require the system know additional information
about how resources are to be requested.

The simplest and most useful model requires that
each thread declare the maximum number of resources
of each type that it may need.

----Safe State---
------------------------
| |deadlock|		|
| ----------  unsafe    |
|			|
|			|
|-----------------------|
|             safe 	|
|			|
------------------------

A state is safe if the system can allocate resource to each proces (up to its maximum)
in some order and stilll avoid a deadlock.

A system is in a safe state only if there exists a safe sequence.

A safe state is not a deadlocked state. Conversly, a deadlocked state
is an unsafe state. No all unsafe state are deadlocks, however.

safe state -> not deadlocked
deadlocked -> unsafe state

An unsafe state my lead to a deadlock. As long as teh state is safe, teh operationg system
can avoid unsafe states. In an unsafe state, the operating system cannot prevent processes from requesting resources
such that a deadlock occurs. The behavior of the processes control unsafe states.

example:
Consider a system with 12 magnetic tape drives and three processe p0, p1, and p2:
		maximum need	|	current needs
----------------------------------------------------------
p0		12		|	5
p1		4		|	2	
p2		9		|	2

At time t0, the system is in a safe state.
the sequence <P1, P0, P2> satisfies the safety condition.

p0 has 5 tape drives at time 0
p1 has 2 tape drives at time 0
p2 has 2 tape drives at time 0

in total 9 tape drives are being used at time 0.
since the system has 12 tape drives total 12 - 9 = 3 tape drives are free.

p1 uses it tape drives then relinquish its resources when it is done there are now 
12 - 7 = 5 tape drives that are free.

p0 uses its tape drive then relinquish its resources when it is done there are now
10 tape drives that are free.

p2 uses its tape drive then relinqush it resources when it is done, there are now
12 tape drives that are free.

example 2:
A system can go from a safe stateto an unsafe state. 
suppose that, at time t1, process p2 requests and is allocated one more tape drive.
the system si no longer in a safe state.

p1 is satisfied and relinquish its resources.
we now have 4 tape drives in hand. p0 and p2 cannot be satisfied
with the current amount resources in hand and thus we are in an unsafe state.

----Resource-Allocation-Graph algorithm
Banker algorithm for multiple instances of each resource type.

Four data structures are need for this algorithm 

Available: A vector of length that represents the availability of resources in the system.

Max: An n x m matrix defines the maximum demand of each thread. 

Allocation: An n x m defines the number of each type currently allocated to each thread.

Need: An n x m matrix indicates the remaining resources
needs of each thread. If Need[i][j] equals k, then thread T_i may need k more instances of resource type R_j to complete its task.a


Safety Algorithm:
1. Let Work and Finish be vectors of length m and n, resptively. Initialize Work = Available. Finish[i] = false for i = 0, 1, ..., n - 1.
2. Find an index i such that both
	a. Finish[i] == false
	b. need_i <= work
	if no such i exists go to step 4
	3. Work = Work + Allocation_i
	   Finish[i] = true;
	   Go to Step 2.
	4. If Finish[i] == true for i, then the system is in a safe state.

This algorithm may require an order of m x n**2 operatiosn to determine whether a state is safe.

Resource-Request Algorithm

----Deadlock Detection----
1. An algorithm that examines the state of the system to determine whether a deadlock has occurred.

2. An algorithm to recover from the deadlock.

dectection-and-recovery scheme require overhead
that includes not only the run-time costs
of maintaining the necessary information and
executing the detection algorithm but also the potential
losses inherent in recovering from a deadlock.

8.7.1 Single Instance of Each Resource Type

If all resources have only a single instance, then we can define a dead-lock detection algorithm that uses a variant of the RAG, called a wait-for graph(WAG). We obtain this graph from the resource-allocation graph by removing the resource nodes and collapsing the appropriate edges.

In a system in which all resources have a single 
instance if the WAG has a cycle then a deadlock
has occurred. To deadlocks, the system
needs to maintain the wait-for graph and periodically
invoke an algorithm that searches for a cycle
i the graph. This operation is O(N**2).

----Several instances of a Resource Type
Similar data structure that are used in the banker's algorithm.

Available: A vector of length m indicates the number of available resources of each type.

Allocation: An n x m matrix defines the number of resources of each type currently allcoated to each thread

Request: an n x m matrix indicates the current request of each thread. If request[i][j] equals k, then then thread T_i is requesting k more instances of resource type R_j.

8.7.3 Detection-Algorithm usage:
When should we invoke the detection algorithm as it is a costly operation.

1. How often is a deadlock likely to occur?
2. How many threads will be affected by deadlock when it happens?

One way is to invoke the algorithm for every
resource request.

Another way is to invoke the algorithm after every
defined interval. Once per hour or when the 
CPU utilization drops below 40 percent.
---------------------------------------recovery from deadlocks--------------------------------------------------

When a detection algorithm determines that a deadlock exists, several alternatives are available. One possibility is to inform the operator that a deadlock has occurred and let the operator deal with the deadlock manually.

another system is to let the system recover from the deadlock.

two options for breaking a deadlock.

1. abort one or more processes
----------two methods that reclaim all resources allocated to terminated processes.

---------abort all deadlocked processes.
costly but will break the deadlock cycle. processes which were almost complete will lose all their work and will have to start from scratch on task

---------abort one process at a time until the deadlock is eliminated
costly because we have to abort a process then run a 
dead-lock detection algorithm again to determine if the
processes are still deadlocked.

deciding which process to abort is a difficult question
and requires many factors, some of these are:

1. What the priority of the process is

2. How long the process has computed and 
how much longer the process will compute before
completing its designated task

3. How many and what types of resources the process
has used

4. How may more resources the process needs in order
to complete

5. How many processes will need to be terminated.

-------------resource preemption
forcefully take resources from processes
and give them to other processes until the
deadlock cycle is broken.

if this is the option to deal with the deadlock,
then three issues need to be addressed.

1. selecting a victim. Which resources and
which processes are to be empted?

2. Rollback. need to restart that process possibly
since it has lost the resources it needed to finish
its execution 

3. Starvation. How do we ensure that starvation
will not occur? It can happen that we continously 
chose the same person as a victim. the most common
solution is to include the number of rollbacks in 
the cost factor.

