The operating system takes physical resources and virtualizes them. It handles issues related to concurrency.
Stores files persistently.

--------------------------------Process------------------
A process is a running program. A program is just
a bunch of instructions and some data sitting in disk.It
is an operating system which brings it to life.

Often we are running more than one program at once.
examples code be listening to music while having
your browser open and using your favorite IDE.

In order to virtualize the CPU to give the illusion
of many CPU's the OS applies time-sharing of the CPU.
We run one program then stop it and run another one and
so on.

mechanisms: low-level methods that implement a 
needed piece of functionality.

address pspaces is the memory that a process can 
address. part of the machine state of a process are
registers.

parts of the machine state:
program counter otherwise known as an instruction pointer
this is what tells us which instrucdion of the program will execute next

stack pointer

-----------------------------Process API-----------------

must be included in the interface of any OS.

Create: OS must have methods to create new processes.
when you double click on a application icon the os
is invoked to create a new process to run the program.
Destroy: OS must have methods to terminate processes.
Wait: OS must hav emthods to wait for process to stop running.
Miscellaneous Control: Methods such as suspending
a process and resuming it.
Status: Methods to get the status information of 
a process.

--------------------------Process States------------------
A process can in different states during its lifetime.
A process can be in one of three states:

- Running: In the running state, a process is running on  a processor. This means it is executing instructions.

- Ready: In the ready state, a process is ready to run but the OS has not choosen to let it run at the given moment.

- Blocked: Process has performed some operation that doesn't allow it continue until some event has taken place.


Process state of two processes that only use the CPU

time		Process0		Process1
1		Running			Ready
2		Running			Ready
3		Running			Ready
4		Running			Ready
5		Done			Running
6		Done			Running
7		Done			Running
8		Done			Running


------------------------Process API----------------------

the fork system call is to create a new process.
It create an almost exact copy ofthe calling process.
the process comes into life as if it had been called
fork() itself.

----------------------Threads----------------------------
threads ia like a separate process execept they share
the same address space and thus can access the same data.

One reason we would want to use threads is parallelism.
you can speed up a program by using multiple threads 
to do a portion of the work.

Another reason is to avoid blocking program progress
due to slow I/O.  While one thread in your program waits
another thread can be switched to which are ready to run.
threads share an address space and thus it is easy
to share data.

working with threads make application more rich but
also more complicated.

race condition: the result depends on the timing
execution of the code.

critical section: piece of code that accesses a share
variable and must not concurrently executed by
more than one thread.

a critical section is piece of code that accesses
a shared resource, usually a variable or data structure.

a race condition: arises if multiple
thread of executation enter the critical section at
roughly the same tiem. both
attempt to update the shared data structure, leading
to unforseen results.

mutual exclusion:
the proprerty that if one thread is executing
within the critical section no other thread is allowed
to do so.

the operating system was the first concurrent program.
-------------------------synchronization-------------------
A cooperating process is one that can affect or be affected by other processes executing in the system.

                        cooperating processes can either

                       /                                \
                      /                                  \
                     /                                    \
                    /                                      \
                   /                                        \
                directly share a logical address             or be allowed to share data only through files or messages
                space. (both code and data)

the problem is that concurrent access to shared data may result in data inconsistency. If multiple processes try to 
access and modify the same data it could lead to data inconsistency. This is why we need process sychronization.

Producer-Consumer problem
A producer process produces information taht is consumed by a consumer process.

For example, a compiler may produce assembly code, which is consumed by an assembler.
The assembler, in, may produce object modules, which are consumed by the loader.

-One solution to the producer-consumer problem uses shared memory.

-To allow producer and consumer processes to run concurrently, we must have available a buffer of items that
can be filled by the producer and emptied by the consumer

-This buffer will reside in a region of memory that is shared by the producer and consumer processes

-A producer can produce one item while the consumer is consuming another item

-The producer and consumer must be sychronized, so that the consumer does not try to consume one item that has not yet
been produced


                                    two kinds of buffers


        Unbounded buffer                                        Bounded Buffer
                |                                                       |
                |                                                       |    
                |                                                       |
                |                                                       |
                |                                                       |
        Places no practical limit                                Assumes a fixed buffer size. In this
on the size of the buffer. the consumer may                      case, the consumer wait wait if the
have to wait for new items, but the producer                     buffer is empty, and the producer must wait if the buffer is b full.

counter variable = 0
counter will keep track of the number of items in the buffer

counter is incremented every time we add a new item to the buffer counter++
counter is decremented every time we remove one item from the buffer counter--

example:
-Suppose that the value of the vaiable counter is currently 5
-The producer and consumer processes execute the statement "counter++" and "counter--" concurrently.

-following the execution of these two statements, the value of the variable counter may be 4, 5, or 6

-the only correct result, though is counter == 5, which is generated correctly if the poducer and consumer execute separately

counter++ may be implemented in machine language as:

        register_1 = counter
        register_1 = register_1 + 1
        counter = register_1

counter-- may be implemented in machine language as:

        register_2 = counter
        register_2 = register_2 - 1
        counter = register_2

table of how the process takes place

t0      producer        execute     register_1    =   counter                  {register_1=5}
t1      producer        execute     register_1    =   register_1 + 1           {register_1=6}
t2      consumer        execute     register_2    =   counter                  {register_2=5}
t3      consumer        execute     register_2    =   register_2 - 1           {register_2=4}
t4      producer        execute     counter       =   register_1               {counter=6}
t5      consumer        execute     counter       =   register_2               {counter=4}

we arrived at this incorrectg state because we allowed both processes to run concurrently.

A situation where several processes access and manipulate the same data concurrently and the outcome of the
execution depends on the particular order in which the access takes place, is called a race condition.

We want the resulting changes not to interfere with one another, hence we need process synchronization.


---------------------------------------------The Critical Section-Problem--------------------------------------
consider a system consisting of n processes (p0, p1, ..., pn)
each process has a segment of code, called a critical section.
in which the process may be changing common variables, updating a table, writing a file, and so on.
when one process is executing in its critical section, no other process is to be allowed to execute in its critical
section. that is, no two other processes are executing in their critical sections at the same time.
that is, no two processes are executing in their critical sections at the same time.

the critical-section problem is to design a protocol that the processes can use to cooperate. they should not
try to manipulate shared data concurrently.

the critical-section problem is to design a protocol that the processes can use to cooperate.

-Each process must request permission to enter its critical section
-The section of code implementing the request is the entry section
-The critical section may be followed by an exit section
-The remaining code is the remainder section

general structure of a process

do{
    entry section
        critical section
    exit section
        remainder section
} while(true);

A solution to the critical-section problem must satisfy the following three requirements:

1. Mutual Exclusion:

If process Pi is executing in its critical section, then no other processes can be executing in their critical section

2. Progress

If no process is executing in its critical section and some processes wish to enter their critical section, then only
those processes that are not executing in their remainder sections can be participate in the decision on which will
enter its critical section nexts, and this selection cannot be postponed indefinitely.

3. Bounded waiting

There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after
a process has made a request to enter its critical section and before that request is granted.

----------------------------------------Peterson Solution---------------------------------
-A classic software-based solution to the critical-section problem
-May not work correctly on modern computer architecture
-Help us to understand the problem at a software implementation level.

Peterson's solution is restricted to two processes that alternate execution between their critical sections
and remainder sections. Let's call the processes Pi and Pj.

Peterson's solution requires two data items to be shared between two processes.

int turn						boolean flag[2]
   |								|
   |								|
   |								|
Indicates whose turn it is to				Used to indicate if a process is ready to enter its
enter its critical section 				critical section.

examples:
turn == i tells us it is process_i turn 		flag[j] == true tells us process j is ready 
to enter its critical section.				to enter its critical section.


Peterson's solution is like a humble algorithm, when it has the chance to enter the critical section it 
gives the turn to the other process. 

analogy:
you and your friend are waiting for the bus and then when it comes you allow you friend to go first and wait
until he enters the bus before you yourself enters the bus.

structure of pi peterson solution

do{
	flag[i] = true; // Process pi is ready to enter its critical section
	turn = j; // algorithm is a humble algorithm so will give turn to process pj
	while(flag[j] && turn[j] == j); // wait while process pj is in its critical section

	critical section

	flag[i] = false; // set the flag as false as the process pj has completed its critical section.

	remainder section
} while(true);


structure of process pj in peterson's solution

do{
	flag[j] = true;// process pj wants to enter its critical section
	turn = i; // process pj is humble and offer the turn to process pi
	while(flag[i] && turn == i); // process pj will wait for process pi 

	critical section

	flag[j] = false; // done with the critical section

	remainder section
} while(true);


example: 
both try to enter the critical section at the same time.
process pi enters first and sets its flag[i] to true and give the turn to process pj.

process pj enters seconds and sets if flag[j] to true and give the turn to process pi.

since turn == i and flag[i] == true, process pi will enter its critical section.
flag[i] will becomes false once process pi exits its critical section so now 
process pj is allowed to enter its critical section.


The peterson solution satisfies all three requirements for the critical section problem

1. mutual exclusion is preserved

2. process is preserved

3. bounded waiting is preserved

peterson solutions is only restricted to two processes and is not guaranteed to work on modern computer
architecture


--------------------------------------Test and Set Lock--------------------------------------------
-Hardware solution to the synchronization problem
-There is shared lock variable which can take on values either, 0 or 1.
-0 means unlocked and 1 means locked
-Before entering into the critical section, a process inquires about the lock.
-If it is locked, it keeps on waiting till it becomes free
-If it is not locked, it sets the lock and executes the critical section

example:
room that only one person can be in at a time. checks if the room is locked or unlock.
only when person who is currently in the room and finishes work unlocks the room can another person
use the room.

definition of test_and_set() function.

boolean test_and_set(boolean* target){
	boolean rv = target;
	*target = TRUE;
	return rv;
}

this function is an atomic operation. all the 
instruction are done and cannot be interrupted.

example:

initially the lock is set to zero.

process pi executes first.
it call the test_and_set function on the lock.
since the lock is 0 the return while will be zero and
it will continue onto the critical section. process pj
will wait in the while loop until process pi exits the critical section
and sets the lock value back to 0.

process pi

do {
	while(test_and_set(&lock));

	// do nothing
	// critical section
	lock = false;
	//remainder section	
} while(true);

advantage of current solution

satisfies mutual-exclusion.

does not satisfies bounded waiting.

process pi enters and complete critical section while process pj waits.
when process pi is done another process pk enters and now process pj still have to waits.
this could happen indefinitely.leads to starvation of process to critical section.

---------------------------------------Semaphore---------------------------------------------------
-Software based solution to the sychronization problem.

-Semaphore proposed by Dijkstra, is a technique to manage concurrent processes by using a simple integer value,
which is known as a semaphore.

-Semaphore is simply a variable which is non-negative and shared between threads. this variable is used to solve
the critical secion problem and to achieve process synchronization in the multi-processing environment

-A semaphore S is an integer variable that, apar from initialization is accessed only through two
standard atomic operations: wait() and signal();

wait() -> [from the Dutch word proberen, which means 'to test']
signal() -> [from the Dutch word verhogen, which means 'to increment']

definition of wait();
P(Semaphore s){
	while(s <=0);
	// no operation
	s--
}

if the value of s > 0 greater than 0, then s will be decremented and enter the critical section.
it decrements the value of s to let other other processes to know that the value of the semaphore has been decreased.

definition of signal();
V(Semaphore s){
	s++;
}

to tell other processes that the process is done using the resource.


all the modification to the integer value of the semaphore in the wait() and signal() operatoins
must be executed indivisbly, 
that is, when one process modifies the semaphore value, no other process 
can simulatenously modify that same semaphore value.

1. Binary Semaphore
The value of a binary semaphore can range only between 0 and 1. On some systems, binary semapohres are known as mutex locks,
as they are locks that provide mutual exclusion.

the value of a semaphore is initialized to 1.


2. Counting Semaphore
Its value can range over an unrestricted doman. It is used to control access to a resource that
has multiple instances. 

two instances of 2 for resource with count of 2
S = 2

process 1 will enter the loop and then decrement the value of S to 1

process 2 will enter the loop and then decrement the value of S to 0

process 3 will enter the loop and then will stay in loop until process 1 or
process 2 release the resource.

-------------------------------------Disadvantage of Semaphores-------------------------------------------------
-The main disadvantage of the semapohre definition that was discussed is that it requires busy waiting.

- While a process is in its critical section, any other process that tries to enter its critical section
must loop continously in the entry code.

-busy waiting wastes CPU cycles that some other process might be able to use productively.

-this type of semaphore is called a spinlock because the process 'spins' while waiting for the lock.

to overcome the need for busy waiting, we can modify the definiton of the wait() and signal() semaphore operations.

-When a process executes the wait() operations and finds that the semapohre value is
not positive, it must wait

--however, rather than engaging in busy waiting, the process can block itself

--The block operation places a process into a waiting queue associated with the semapohre, and the sate of the process is switched to the waiting state.

--the control is transferred to teh CPU schedule, which select another process to execute 

--------------------------------------Deadlocks and Starvation--------------------------------------
-The implementation of a semaphore with a waiting queue may result in a situation where
two or more processes are wiaitng indefintey for an event tha can be caused only one of the 
waiting processes
-The event in question is the execution of a signal() operation. When such as state is reached, these processes are said
to be deadlocked.

p0			p1	
wait(S);		wait(Q);
wait(Q);		wait(S);
   .			   .
   .			   .
   .			   .
signal(S);		signal(Q);
signal(Q);		signal(S);

---------------------------------------------Classic Problems of Syncrhonization-------------------------------------
					          (The bounded-buffer problem)

The bounded buffer problem is one of the classics problems of synchronization

There is a buffer of n slots and each slot is capable of storing one unit of data.

There are two processes running, namely, producer and consumer, which are operating
on the buffer.

-there are two processes running, namely, producer and consumer, which are operating on the bufffer

-the producer tries to insert data into an empy slot of the buffer.
-the consumer tries to remove data from a filled slot in the buffer.
-the producer must not insert data when the buffer is full
-the consumer must not remove data when teh buffer is empty.
-the producer and consumer shoud not insert and remove data simultaneously

we make use of three semaphores:

1. m(mutex) a binary sempahore which is used to acquire and release the lock
2. empty, a counting semaphore whose initial value is the number of slots in the buffer, since, initially all slots are empty.
3. full, a counting semaphore whose initial value is 0.  

producer

do{
	wait(empty); // wait until empty > 0 and then decrement empty
	wait(mutex); // acquire lock
	/* add data to buffer */
	signal(mutex); // release lock 
	signal(full); // increment full because the producer has added one data to the buffer
} while(TRUE);

// consumer 
do {
	wait(full); // wait until full > 0 and then decrement 'full'
	wait(mutex);// acquire lock
	/* add data to buffer */
	signal(mutex); // release lock
	signal(empty); // increment empty because the consumer has removed one data to the buffer
} while(TRUE);

three problems we have stated have been solved.
the producer should not produce when the buffer is full.
the consumer should not consume when the buffer is empty.
the consumer and producer should not simulatenously consume and produce.
-----------------------------------READER-WRITERS PROBLEM-------------------------------------------
-A database is to be shared among several concurrent processes.
-Some of these processes may want only to read the database, wherease others may want to
update
-We distinguish between these two types of processes by referring to the former as readers and the latters
as writers
-obviously, if two readers access the shared data simultaneously, no adverse affect will result.
-however, if a writer and some other thread(either a reader or a writer) access the database simulatenously,
chaos may ensue.
-To ensure that these difficulties do not arise, we require that the writers have
exclusive access to the shared database.
this sychronization is referred to as the readers-writers problem.

we will make use of two semaphores and an integer variable.
1. mutex, a semaphore(initialized to 1) which is used to ensure mutual exclusion when readcount is updated i.e
when any reader enters or exit from the critical section.
2. wrt, a semaphore(initialized to 1) common to both reader and writer processes.2. wrt, a semaphore(initialized to 1) common to both reader and writer processes.
3. integer that keep tracks of how many variables are reading the data.

writer process
do{
	/* writer requests for critical section */
	wait(wrt);
	/* perform the write */
	// leaves the critical section 
	signal(wrt);
} while(true);

reader process
do{
	wait(mutex);
	readcnt++; // the number of readers has now increased by 1
	if(rcnd == 1)
		wait(wrt); // this ensure no writer can enter if there is even one reader
	signal(mutex); // other readers can enter while this current reader is inside teh crticial section
/*	current reader perform reading here */
	wait(mutex);
	readcnt--;
	if(readcnt == 0)
		signal(wrt);
		signal(mutex);

} while(true);

---------------------------------The dining-philophers problem-------------------------------------
five philosphers in two possible states, thinking and eating.
philospher needs to have two chopsticks in order to eat. chopsticks
are limited. 5 chopsticks for 5 philosphers.

				philospher either
			/			 	\
		      thinks 				eats
			|				 |
			|				 |
		   when a philospher			when a philospher gets hungry he tries to pick up the forks that are cloest to him, a philospher may pick up only one fork at a time. cannot pick up fork on the hands of a neighbor.
		   thinks he does not
		   interact with his colleagues.

no two adjacent philosphers can eat at the same time. this is a problem of resource allocation.
One simple soluition is to represent each fork/chopstick with a semaphore.
a philospher tries to grab a fork/chopstick by executing a wait() operation on that semaphore.
he releases his fork/chopsticks by executing the signal() operation on the appropiate semaphores.

		thus the shared data are
		semaphore[5];
where all the elements of chopsticks are initialized to 1.
so we are using binary semaphores.

structure of philospher i
do{
	wait(chopstick[i]);
	wait(chopstick[((i + 1) % 5)]); // uses modular because circular table
	// eat
	signal(chopstick[i]);
	signal(chopstick[(i + 1) % 5]);
	// think
} while(TRUE);

although this solutaiotn guarantees that no two neighboars are eating simulatenously.

all five become hungry simulatenously and thus all elements of chopsticks will be zero.

when a each philospher tries to grab his right chopstick he will be deadlocked forever.

----------------------------------------Monitors----------------------------------------------------
-A high-level abstraction that provides a conventient and effective mechanism for process
sychronization
-A monitor type presents a set of programmer-defined operations that provide mutual exclusions
within the monitor.
-The monitor type also contains the declaration of variables whose values define the satte of an instance of that type,
along wit the bodies of procedures or functions that operare on that variables.


syntax of a monitor

monitor monitor_name
{
//shared variable declaration
procedure p1(...){

}

procedure p2(...){

}

procedure pn(...){

}

initialization code(...){

}
}

-A procedure defined within a monitor can access only
those varaibles declared locally within the monitor and its parameter.
-Similary, the local variables of a monitor can accessed by only the local procedures.
-The monitor construct ensures that only one process at a time can be active within the monitor
-conditional construct-			condition x, y;
the only operations that can be invoked on a condition variable are wait() and signal().

the operation x.wait(); means that process invoking this opoerationg is suspended until another process invokes
x.signal();

the x.signal() operations resumes exactly one suspended process.



queue		shared data				entry queue
associated
with x,y

		operations

		intialization code

Dining-philosphes solution monitors 

-We now illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosphers problem.

-This solution imposesthe restriction that a philospher may pick up his chopsticks only if both are available.

-to code this solution, we need to distinguish among three states n which we may find a philospher.
for this purpose, we introduce the following data structures.

		enum {thinking, hungry, eating} state[5];

-philospher i can set the variable state[i] = eating only if his two neighbors are not eating:
(state[(i+4) % 5] != eating) && (state[(i+1) % 5] != eating)

-we also need to declare condition self[5]; where philospher i can delay himself when he is hungry 
but is unable to obtain the chopsticks he need.


Monitor dp{
	enum {thinking, hungry, eating} state[5];
	
	void pickup(int i){
		state[i] = hungry;
		test(i);
		if(state[i] != eating)
		self[i].wait();
}
	void putdown(int i){
		state[i] = thinking;// done eating
		test((i + 4) % 5);// give the neighbors chance to eat.
		test((i + 1) % 5);
}

	void test(int i){
		if((state[(i + 4) % 5] != eating) && (state[i] == hungry) && state[(i + 1) % 5] != eating)
		{
		state[i] = eating;
		self[i].signal();
	}
}

			
		// initially all philosphers are in the thinking state.
		initialization-code(){
			for(int i = 0;i < 5;i++){
				state[i] = thinking;
		}
	}
}

when one philospher finishes the philospher to the left and right are tested, this solution does not have
dead lock a philospher need to have both chopsticks avaiable before they are allowed to eat. only one process
at is allowed in the monitor at a time.

process synchronization question

Consider the methods used by processes P1 and P2 for accessing their critical secions whenever needed, as given below.
the initial values of shared boolean variables s1 and s2 are randomly assigned.

p1 methods
while(s1 == s2);
critical section
s1 = s2;

p2 method
while(s1 != s2);
critical section
s2 = not(s1)

Which one of teh following statements describes the properties achieved?

1. mutual exclusion but not progress
2. progress but not mutual exclusion
3. neither mutual exclusion nor progress
4. both mutual exclusion and progress

this will achieve mutual exclusion but not progress.
one process can be postponed indefinitely.

keep in mind definitions and analyze code to solve theses problems.

question 2

The following program consists of 3 concurrent processes and 3 binary semaphorse the semaphores are initialized to
s0 = 1, s1  = 0, s2 = 0

process p0 	process p1 		process p2
while(true){	wait(s1);		wait(s2);
wait(s0);	release(s0);		release(s0);
print(0);
release(s1);
release(s2);
}

How many times will proces p0 print '0'?

a. at least twice

b. exactly twice

c. exactly thrice

4. exactly once

b exactly twice is my current answer

c is the current answer. did not take into account the case when

p0 runs then p1 then p0 then p2 then p0.

question

process p1
while(true){
wants1 = true;
while(wants2 == true);
wants1 = false;
} /* remainder section */

process p2
while(true){
wants2 = true;
while(wants1 == true);
/* critifcal section */
wants2 = false;
}
}

here, want1 and wants2 are shared varialbes, which are initialized to flase.
which one of the following statements is true about the above construct?

a. it does not ensure mutual exclusion

b.  it does not ensure bounded waiting

c. it requireds that processes enter the critical section in stric alternation

d. it does not prevent deadlocks, but ensure mutual exclusion.

d is the correct answer.

tip: see the things that you know about and check what happens.

question 

consider three concurrent processes p1, p2, and p3 as shown below, which access a shared variable D that
has been initialized to 100.

p1		p2		p3
--------------------------------------------
	.	.		.
	.	.		.
	.	.		.
D = D + 20	D = D - 50	D = D + 10
	.	.		.
	.	.		.

The processes are executed on a uni-processor system running a time-shared operating system. If the minimum and 
maximum possible values of D are the process have completed execution are X and Y respectively, then the value of
y-x is ___

130

1.80
2.130
3.50
4. none of these

1. is the correct answer.

----------------------------------------------------------------------------------------------------

    while(true){
        /* produce an item in next produced */
        while(count == BUFFER_SIZE);

        buffer[in] = next_produced;
        in = (in + 1) %  BUFFER_SIZE:
        count++;
}


    while(true){
        while(count == 0);
        next_consumed = buffer[out];
        out = (out + 1) % BUFFER_SIZE;
        count--;
}


possible implementation of count++

register_1 = count;
register_1 = register_1 + 1;
count = register_1;

possible implementation of count--

register_2 = count;
register_2 = register_2 - 1;
counter = register_2;

we can arrive at the incorrect state because we allow both
processes to manipulate the variable count concurrently.

race condition: outcome of the execution depend
on the particular order in which the access takes place.

in order to prevent a race condition we have to make
sure that only one process at a time can access the 
count variable.

criticial section: process is accessing or updating
data that is shared with at least one other process.

we want so that no other process is allowed to execute in
the critical section while another one is executing.

critical section problem: design a protocol that
the processes can use to synchronize their activitity so
as to cooperatively share data.

each process requires permission to enter critical section.

entry section: section of code in the critical section
that implements the permission for a process to enter

solution to critical-section problem must satistfy three requirements.

1. Mutual exclusion: If process pi is executing in its critical section, then no other proesses can be executing in their critical sections.

2. progress: if no process is executing in its critical section and some processes wish to enter their critical section. then only entries not executing remainder section can decide which will enter its critical section.

3. bounded waiting: bound on number of tiems other processes
are allowed to enter their critical section after a process
has made a request to enter its critical section.

while (true){
    entry section
            critical section
    exit section
            remainder section
}


peterson solution:

int turn;
boolean flag[2];
while(true){
        flag[i] = true;
        turn = j;
        while(flag[j] && turn == j);
        /** critical section */
        flag[i] = false;
        /*remainder section */
}

peterson solution does not work on mult-threaded applications.

software-based solutions are not guaranteed to work on modern
computer architectures.

memory model:

1. strongly ordered: when a memory modification on one processor is immediately visible to all other processors.

2. weakly ordered: where modifications to meomry on one processor may not be immediately visible to other processors

memory modifications are visible to threads running on 
other processors: memory barriers.

while(!flag)
    memory_barrier();
print x;

x = 100;
memory_barrier();
flag = true;

memory barriers are considered very low-level operations.

hardware instructions

atomically: one uninterruptible unit.

boolean test_and_set(boolean* target){
    boolean rv = *target;
    *target = true;
    return rv;
}

do {
    while(test_and_set(&lock));
    /* do nothing */

    /* critical section */

    lock = false;

    /* remainder section */

} while(true)

int compare_and_swap(int *value, int expected, int new_value){
    int temp = *value;
    if(*value == expected)
        *value = new_value;
    return temp;
}

while(true){
        while(compare_and_swap(&lock, 0, 1) != 0);
        /* critical section */
        lock = 0;
        /* remainder section */
}

this algorithm satisfies the mutual exclusion property but
does not satisfy the bounded waiting property.

the code segment below satisfies all the critical section requirements

while(true){
    waiting[i] = true;
    key = 1;
    while(waiting[i] && key == 1)
        key = compare_and_swap(&lock, 0, 1);
    waiting[i] = false;
    
    /* critical section */

    j = (i + 1) % n;
    while((j != i) && !waiting[j])
        j = (j + 1) % n;

    if(j == i)
        lock = 0;
    else
        waiting[j] = false;
    /** remainder section */
}

atomic variable is used to ensure mutual exclusion
in situations where there may be a data race on 
a single variable while it is being updated as when a counter
is incremented. 

increment(&sequence);

void increment(atomic_int *v){ 
    int temp;
    do {
        temp = *v;
    } while(temp != compare_and_swap(v, temp, temp + 1);
}

still not a viable solution to the consumer producer problem.

mutex lock

mutex is short for mutual exclusion

mutex lock has a boolean variable available whose
value indicates if the lock is available or not.
If the lock is available, a call to acquire() succeeds.
and the lock is then considered unavailable. A process
that attempt to acqurie an unavailable lock is blocked
until the lock is released.

while (true){
    acquire lock
        critical section
    release lock
        remainder section
}

// the definition of the acquire function

acquire(){
    while(!available);
    available = false;
}

// the definition of the release function
release(){available = true;}

calls to either acquire() or release() must be performed atomically.

busy waiting: while a process is in its critical section,
any other process that tries to enter its critical section
must loop continously in the call to acquire().

this type of mutex lock is also called a spin-lock.

advantage: no context switching is needed.
disadvantage: processes have to loop continously while
waiting to enter the critical section.

mutex locks are considered the simplest of synchronization tools

semaphores: integer variable that is only accessible
through the wait and signal system calls..

wait(s){
    while(s <= 0);
    s--;
}


signal(s){s++;}

when one process
modifies the semaphore value, no other process can simultaneously modify
that same semaphore value.

counting semaphore: can range over an unrestricted domain.

binary semaphores: can range only between 0 and 1.

binary semaphores behave similary to mutex locks.

systems that do not have mutex locks can use binary semaphors
for mutual exclusion.

counting semaphores are used to control access to a given
resource consiting of a finite number of instances.

the semaphore is initialized to the number of resources available.

when a process releases a resource, it performs a signal() operation.

each process that wishes to use a resource performs a wait()
opeartion on the semaphore.

when the count for the semaphore goes to 0, all resources are being used. when a process is done with a resource it 
performs a signal.
After that, processes that wish to use a resource will block until the count becomes greater than 0.

s1;
signal(synch);
wait(synch);
s2;


the current implementation suffers from busy waiting.

we modify the definition so that we no longer are busy waiting.

typedef struct{
    int value;
    struct process* list;
} semaphore;

wait(semaphore *s){
        s->value--;
        if(s->value < 0){
            add this process to s->list;
            sleep();
        }
}


signal(semaphore *s){
    s->value++;
    if(s->value <= 0){
            remove a process P from s->list;
            wakeup(P);
    }
}

----------------------Monitors---------------------------------

Timing errors can still occur when mutex locks or semaphores are used.

A monitor type is an ADT that includes a set of programmer-defined operations that are provided with mutual exclusion
within the monitor. 

errors can be caused by a programmer.

// problem 1
signal(mutex);

/*
    critical section

*/

wait(mutex);

// problem 2

wait(mutex);

/*
    critical section
*/

wait(mutex);

// problem 3
one of the calls of wait or signal is ommitted.

monitors are high-level constructs of synchronization tools.

Monitor is an abstract data-type

by themselves are not sufficient enough to handle sychronization neeeds constructs like conditioning.

condition x, y;

the only operation that can be invoked on a condition variable
are wait() and signal(). The operation

    x.wait();

mean that the process invoking this operation is suspended until
after another process invokes

    x.signal();


x.signal() is called by a process p while a process q is suspended.

2 possibilities exist

1. signal and wait

2. signal and continue

many programming languages have incorporated monitors.

any function is replaced by

wait(mutex);
/*
    body of F
*/
if (next_count > 0)
    signal(next);
else
    signal(next);

x_count++;
if (next_count > 0)
    signal(next);
else
    signal(mutex);
wait(x_sem);
x_count--;


if (x_count > 0){
    next_count++
    signal(x_sem);
    wait(next);
    next_count--;
}
-------------implementing a monitor using semaphores------------
use a binary semaphore for each monitor.

using the signal and wait approach

wait(mutex);
/*
*/

if(next_count > 0)
    signal(next);
else signal(mutex);



---------------------------------synchronization examples-----------












-------------------------------Operating System Interfaces -------------------
1. User interface
    1. command line interface
    2. Graphical User Interface

2. Program execution    

3. I/O operations

keyboard
mouse
monitor
printer
speaker
other input
other input
other output

4. File System Manipulation
access restriction is controlled by operating systems

5. Communications

processes that communicate with each other so their execution
can be done in an effective way.

6. Error detection

important service provided by the operating system

7. Resource Allocation

A process keeps waiting for a resource but never gets it
Process gets resource but never releases it

8. Accounting

Which user uses and how much resources and of what type
keeping usage statistics.

9. Protection and Security

Processes are executing at the same time, a process should
not be able to interfere with other processes operations.


----------------------------------User Operating System Interface-----------------------------------------------------

There are two fundamental approaches for users to interface with
the operating system.

1. Provide a command-line interface or command interpreter that allows users to directly enter commands that are to be performed by the operating system.

2. Allow the user to interface with the operating system via a graphical user interface or GUI.

Some operating systems include the command interpreter in the kernel

others such as windows XP and UNIX, treat the command interpreter as a special program.

On systems with multiple interpreter to choose from the interpreters are know as shells

bourne-shell
c shell
bourne again shell
korn shell

System Calls

System Calls provide a interface to the services made available
by an operating system. 

User mode and kernel mode

two modes in which a program can execute.

user mode does not have direct access to memory or hardward
or such resources

kernel mode is privileged mode.

kernel mode programs that crash will crash entire system.

user mode program crash the whole system will not crash.

user mode is more safe so more program exectute in usermode.

switching from kernel to user and vice verse is known as mode
shifting.

system calls provide an interface to the services made available
by an operating system.

System calls is the programmatic way in which a computer program
requests a service from the kernel of the operating system.

example we want to read data from one file to another

Acquire input filename
write prompt to screen
accept input
Acquire Output filename
Write prompt to screen
Accept Input
Open Input File
If file doesn't exist, ABORT
CREATE Output file
If file exists, ABORT
write completion message to screen
terminate normally

had to use lots of system calls even for very simple tasks.
system calls are always being executed.

These calls are generally avaiable on routines written in
C and C++.

-----------------Types of Systems Calls----------------------

1. Process Control
2. File Manipulation
3. Device Management
4. Information Maintenance
5. Communications

1. Process Control

-end, abort
-load, execute
-create process, terminate process
-get process attributes, set process attributes
-wait for time
-wait even, signal event
-allocate and free memory

2. File Manipulation
-create file, delete file
-open, close
-read, write, reposition
-get file attributes, set file attributes

3. Device Manipulation
-Request device, Release Device
-Read, Write, Reposition
-get device attributes, set device attributes
-logically attach or detach devices

4. Information Maintenance
-get time or date, set time or date
-get system data, set system data
-get process, file, or device attributes
-set process, file, or device attributes

5. Communication
-create, delete communcation connection
-send, receive messages
-transfer status information
-attach or detach remote devices

These are the five major categories we can grouped system calls

--------------System Programs----------------------------------
An important aspect of a modern system is the collection of 
system programs.

-System programs provide a convenient environment for
program development and execution

File Management
-create
-delete
-copy
-rename
-print
-dump
-list

Status information
-date, time
-amount of available memory or disk space
-Number of users
-Detailed performance
-Logging, and debugging information

File Modification modifiying inner content of files

Programming-language support
-compilers
-assembler
-debuggers
-interpreters

program-loading and execution
-absolute loaders
-relocatable loaders
-linkage editors
-overlay editors

debugging systems for either higher-level langauges or machine
language are needed as well.

Communications

-creating virtual connections among processes, users, and computer systems
-allowing users to send message to another's screen
-to browse webpages
-to send electronic-mail message
-to log in remotely or to transfer files from one machine to another.

In addition to system programs, most operating system 
have programs to solve common problems.

--------------Operating System Design & Implementation----------
1st Problem: Defining Goals and specifications
2nd Problem: Type of system

Beyond this highest design level, the requirements may be
much harder to specify.


requirements
-user goals:This system should easy to use, convienent and safe
-system goals: Easy to design, implement, operate

Mechanisms and policies:

Mechanisms determine how to do something

policies what will be done.

One important is the separation of policy from mechanism.

separation of policies and mechanism will make the system
good and flexible.

First operating systems were written in assembly language.

Now most commonly written in higher-level languages 
such as C or C++.

advantages of writing in higher-level language
-the code can be written much faster
-it is most compact
-it is easier to understand and debug
-it is easier to support

-Operating systems vary greatly in their makeup internally

- COMMONALITIES:
    -multiprogramming
    A single user cannot, in general, keep either the CPU or the I/O devices busy at all times
    - multi-programming increases CPU utilization by organizing
    jobs so taht the CPU always has on to execute
    - Job contain code and data
    - All jobs that are to be executed contains job pools
    - cannot load all jobs at once become memory is limited.
    - If we don't have job programming everytime we run a job
      we have to wait until the job is finished completely
      before another job can use the CPU.
    - Multi-programming: One Job uses CPU then switches to IO
      then job 2 uses CPU until Job 1 is done with IO
    - For a operating system to be effecient it should be
      able to ultilize multi-programming
    - Time-sharing(Multi-tasking system)
        -CPU executes multiple-jobs by switching among them
        -Switches occurs so frequently that the users can interact while it is running
        -Time-sharing requires an interactive computer system,
         which provides direct communcication between the user
         and the system
        - A time-shared system allows many users to share the computer
         simultanelously
        - while their is time-gap between system and the user that
          the computer can serve other users.
        -Uses CPU scheduling and muli-programming to provide
         each user with a small portion of a time-share computer
        -Each user has at least on separate program in memory
        -A program loaded in memory and executing is called a "Process"
        
-------------------Structures of Operating Systems-------------
Simple Structure

Application Program
|
|
|
Resident Systems Programs
|                   
|
Device Drivers
|
|
ROM BIOS device drivers

All things above can access elements in the bottom

Monolithic Structure

            
The users


Shells and commands
compilers and interpreters
system libraries

kernel

If you want to add something to this system it would be
difficult to add to it.

Layers Structure

layer can only use layer below another layer.
designing this layered structure is difficult.

system can be slow.

Microkernels

we have microkernel.
implement non-essential fo kernel as programs.

micro-kernel can have performance decrease.
communication can lead to performance decrease.

Modules:

current best way to create modular kernel in objected-oriented
programming.

Each kernel system has a protected interface.Any module can
call any other module. more flexible than layered structure.

One of the best structuring of OS we can have.

-------------------------Virtual Machines-----------------------

-abstract the hardware of a single computer into different 
execution environments. create the illusion of many computers.

-Non-virtual machine has to do one process at a time

-virtual machine can split up so that each process has 
it's own illusionary computer.

implementation

virtual-machine software - runs in kernel mode
virtual-machine itself -   runs in user mode

A virtual has two modes just like a physical machine

1. A virtual user mode
2. A virtual kernel mode

both of which run in a physical user mode

---------------------Operating System Generation & System Boot-----------------

-Design code, and implement an operating system specificallly
for one machine at one site

-Operating systems are designed to run on any of a class
of machines at a variety of peripheral configurations

-The system must then be configured or generated for each
specific computer site, is process sometimes know as 
system generation is used for this

-The following knids of information must be determined by the SYSGEN Program
    - What CPU is to be used?
    - How much memory is available
    - What devices are available
    - What operating-systems options are desireds

-System Boot
    - The procedure of starting a computer by loading the kernel is known as booting the system
    - On most computer systems, a small piece of code known as bootstrap program or bootstrap loader locates the kernel
    - This program is in the form of ROM. ROM 
      needs no initialization and cannot be infected by a computer virus

Firmware - Another type of read-only memory
If you want to change something in the ROM you have to change
everyone on the ROM chip.

EPROM-erasable programmable read-only memory

When the full bootstrap program has been loaded, it can
traverse the file system to find the operating system
kernel, load it into memory, and start its execution.
It is only at this point that the system is said to be
running.

---------------------Process Management-----------------------

- Process can be though of as a program in execution.

- A thread is the unit of execution within a process. A process
  can have anywhere from just one thread to many threads.

A process can have many threads.


--------------------------------Process States------------------

- As a process executes, it changes states

- The state of a process is defined in part by the current activity of that processs

each process may be in the following states

new -> the process is being created

running -> instructions are being executed

waiting -> the process is waiting for some event to occur

ready ->   the process is waiting to be assigned to a processor

terminated ->   the process has finished execution

-------------------------Process Control Block------------------
Each process is represeneted in the operating system by 
a process control block. also called a task control block

process id-every process is identified by a unique id
process state-the particular state a process is in a particular moment
process number-
process counter-address of next line of instruction to be executed
registers-Tells us the registers that are being used in a given process
memory limits
list of open files

I/O status information: Input Output devices assigned to a process

process control block is used to represent a particular process


-----------------------Process Scheduling-----------------------
-The objective of multi-programming is to have some process
running at all times, to maximize CPU utilization.

-The objective of time-sharing is to switch the CPU among processess so frequently that users cn interact with each program with each program while it is running.

-To meet these objectives, the process scheduler selects on
available process for program execution on the CPU.

-For a single processor system, there will never be more than one running process

-If there are more processes, the rest will have to wait until
the CPU is free and can be rescheduled.

JOB QUEUE-As process enter the system, they are put into a job queue,
which consists of all processes in the system.

READY QUEUE-The processes that are residing in main memory
and are ready and waiting to execute are kept on a list called the ready queue.

example
From job queue goes to ready queue. completes execution
and terminates.

from job queues goes to ready queue. executing but then
a more import process comes so the job is swapped out into
partially executed process then ready queue.

-------------------------------Context Switch-------------------
-Interrupts cause the operating system to change a CPU from
its current task to run a kernel routine

-Such operations happen frequently on general-purpose systems

-When an interrupt occurs, the system needs to save the curent context of the process currenlty running on the CPU so that it
can restore the context when its processing is done, essentially
suspending the process and then resuming it.

-The context is represented in the PCB of the process.

-At home reading a book.the process is reading a book. suddenly 
mother comes and asks and you to do something. you are being
interuptted reading a book to help your mother. put book mark
on book and then help your mother with the task. save the current process of what you were doing.

-Switching the CPU to another process requires performing
a state save of the current process and a state restore
of a different process.

This task is known as a context switch. ssave the
state of the process and then restore the state of the 
process.

-Context-switch time is pure overhead, because the system does no useful work while switching

-It speed varies from machine to machine

-typical speeds are a few milliseconds

------------------------Operations on Process-------------------
-A process may create several new processes, via a create-process system call, during the course of execution
-The creating process is called a parent process, and the new processes are called the children of the process
-Each of these new processes may in turn create other process, forming a tree of processes
-from shell you can issue commands like ls and cat.
-When a process creates a new process, two possibilities exist in terms of execution
1.The parent continues to execute concurrently with its children
2. the parent waits until some or all of its children have terminated


There are also two possibilities in terms of te addresss space of the new process:
1.the child proces is a duplicate of the parent process
2. the child process has new program loaded into it


---------------------------Process Termination------------------
-A process terminates when it finishes executing its final
statement and asks the operationg system to delete it
by using the exit() system call.
-Use the exit() call a process ask the OS to terminate the process
-At the point the process may returna status value to its parent process
-All the resources of the process-including physical and
virtual memory, open files and I/O buffers are deallocated
by the operating system.
-Termination can occur in other circumstances as well
-A process can cause the termination of another process vian
an appropiate system call.
-Usually such a system call can be invoked only by the paren to the process that is to be terminated
-Otherwise, users could arbitrarily kill each other's job.

A parent may terminate the execution of one of its children
for a variety of reasons, such as these:

-The child has exceed its usage of some of the resources that
it has been allocated.

-The task assigned to the child is no longer required

-The parent is exiting, and the operating system does not allow a child to continue if its parent terminates

-----------------------------Interprocess Communication---------
-Processses executing concurrently in the operating system
may be either indepenent processes or cooperating processes
-Independent Processes-They cannot affect or be affected by the other processes executing in the system
-Cooperating Processes-They can affect or be affected b teh other processes executing in the system.
-There are several reaons for providing an environment that allows process cooperation

-Information Sharing
-Computation speedup
-Modularity-design system so that we have separate modules
-Convenience-allows users to do multiple tasks at the same time
-Cooperating processes require an itnerprocess communication
mechanism that will allow them to exchange data and information.
-There are two fundamental models of interprocess communication.
-shared memory
-message passing


-In the shared-memory model, a region of memory that is shared by cooperating processes is established
-Processes can then exchange information by reading and writing data to the shared region.
-When one process wants to communicate with another process it will write into shared memory region
-the other process can read that data

--------------------------------Shared Memory Systems-----------
-Interprocess communication using shared memory requires
communicating processes to establish a region of shared memory
-typically, a shared-memory region resided in teh address space
of the process creating the shared-memory segment
-other processes that wish to communicate using this shared-memory segment must attach it to their address space
-normally, the operating system tries to prevent one process from access another process's memory
-shared memory requires that two or more processes agree to 
remove this restriction
-A producer process produces information that is consumed
by consumer process.
-For example, a compiler may produce assembly code, which
is consumed an assembler. the assembler, in turn, may produce
object modules,which are consumed by the loader.
-they have to work concurrently so that consumer and produce
consume in equilibrium
-one solution is to use shared-memory
-buffer of items that are filled by producer and consumed
by consumer.
-The producer and consumer must be syncronized so that the
consumer does not try to consume what has not been produced


two kinds of buffers
-unbounded buffers-no limit ont the size of the buffer. consumer
may have to wait for new items, but the produces can always produce new items
-bounded buffers=assumes a fixed buffer size, in this 
case, the conusmer must wait if the buffer is empty.
and the producer must wait if the buffer is full.


---------------------------Threads------------------------------
A program under execution is a process
each process can have a number of threads
basic unit of CPU utilizaiton

thread is composed of
a thread ID
a program counter
a register set
a stack

It shares with other threads belong to the same process
its code section, data section, and other operating system
resources such as open files and signals.

A traditional / heavyweight process has a single thread of control

If a process has multiple threads of control, it can perform more than one task at a time..

single-threaded process

--------------------
| CODE DATE FILE   |
| registers  STACK |
|                  |
|                  |
|   THREAD         |
|                  |
|                  |
--------------------

As shown in the figure above a program that is a single-threaded
process can only have a single thread of control. There is
only one program counter and one sequence of instructions
that can be carried out at any given time.


multi-threaded process



--------------------
| CODE DATE FILE   |
| r1   r2   r3     |
| s1   s2   s3     |
|                  |
| t1   t2   t3     |
|                  |
|                  |
--------------------

as shown above each thread has their own program counter,
their own registers and stack but they share the code
data and files.

You can look at a process to see the threads associated with
it.

each thread may be doing different tasks

one thread is dedicated to viewing webpage
another page is spent downloading.

the benefits of multi-threading programming can be broken down into four major categories

responsiveness -    multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a length operationg, thereby moving responsiveness to the user.

resource sharing - by default, threads share the memroy and
the resources of the process to which they belong. the benefit
of sharing code and data is that it allows an application
to have several different threads of activity within the same
address space

allocating memory and resources for process creation is costly.
because threads ahre resoucrse of the process to which they
belong, it is more economical.

utlization of multi-process architecture.
multi-threading can be greatlu increased in a mult-processor

Multi-threading models and Hyper-threading

1. User-threads - Supported above the kernel and are managed without kernel support
2. Kernel-threads - Supported and managed directly by the operating system

Ultimately, there must exist a relationship between user threads and kernel threads.

Many-to-One model

kernel thread has many user threads.

maps many user-level threads to one kernel thread

thread management is one by the thread library in user space, so it is effecient.

the entire process will block if a thread makes a blocking system call

because only one thread can accesss the kernel at a time, multiple threads are unable to run in parallel on multi-processor

one-to-one model

one user thread is mapped to one kernel thread

maps each user thread to a kernel thread
more concurrency
allows multi-threads to run in parallel on mult-processors

-creating user thread requires corresponding kernel thread
-restrict the number of threads supported by system.

Many-to-Many model

many user threads are mapped to many user threads

many user threads to smaller many threads
developers can create as many user threads as necessary
and the corresponding kernel. threads can run in parallel
on multiprocessor.

Also when a thread performs a blocking system calls, the kernel
can schedule another thread for execution

-----------------fork() and exec() system calls-----------------

fork(): The fork() system call is used to craete a separate, duplicate process.

create exact replica of that process. separate duplicate
process will be create.a different process.
duplicate process will be called a child process.

exec(): when an exec() system call is invoked, the program
specified in the parameter to exec() will replace the entire
process - including all threads.

replace a process with another. they will have the same process
id because we are replacing a process.


---------------------------threading issues-------------

duplicate all threads or duplicate thread taht invoked
the fork() system call. If a thread invokes
the exec() system call, the program specified in 
the parameter to exec() will replace the entire
process---including all the threads.

which of the two version of fork() to use depends on
the applications.

if exec() is called immediately forking

then duplicating all threads is unnecessary,
as the program specified in the parameters to exec()
will replace the process. In this instance, duplicating
only the calling thread is appropriate.

If the separate process does not call exec() after forking
then the separate process should duplicate all threads.

this is how we use fork depending on the different 
scenarios that we could have.

------------------Thread Cancellation--------------------

Thread cancellation is the task of termination a thread before it has been completed. Before a thread could complete its execution it was terminated.

example of thread cancellation 
If multiple threads are concurrently searching
through a database and one thread returns the result,
the remaining threads might be canceled.

When a user presses a button on a web browser
that stops a web page from loading any further,
all threads loading the page are canceled.

A thread that is to be canceled is referred to as a 
the target thread.

Cancellation of a target thread may occur in 
two different scenarios 

1. Asychonronous cancellation: One thread immediately
terminates the target thread.

the thread has no power, it is cancelled by 
some other thread.

2. Deferred cancellation: The target thread 
peridoically check whether it should be terminate,
allowing it an opportunity to terminate itself
in an orderly fashion.

the thread has some power, it will not be directly
cancelled by some other thread.

Where the difficulty with cancellation lies

1. Resources have been allocated to a cancelled thread

2. A thread is cancelled while in the midst of
updating it is sharing with other threads.

Often, the OS will reclaim system resources rom
a canceled thread but will not reclaim all resources.

therefore, canceling a thread asynchronously
may not free a necessary system-wide resource.

deferred cancellation is safer. The target thread
has checked itself and it is in a position to 
be cancelled. it will not cancel itself until it
is at a point where it can canceled safely.

deferred cancellation is the preferred method.

-----------------Thread review----------------------------
A thread is like a worker in a toy shop
---is an active entity
---executing unit of toy order
---works simultaneously with others
---many workers completing toy orders
---requires coordination
---sharing of tools, parts, work stations


---is an active enity
----executing unit of a process
----works simultaneously with others
----many threads executing
----requires coordination
----sharing of a I/O devices, CPU, Memory

Why are threads useful?

Each thread execute same code for different subset of input
matrix. by parallization we can achieve speedup. different
thread can work on different parts of the program. 

parallelizzation -> speedup
specilizaton -> hot cache
effecieny -> cheaper IPC

benefits to applications and OS

multi-thread kernels
thread working on behalf of apps
os-level services





